[["strat.html", "Other stratification methods K-means How many clusters? Optimum stratum boundaries Sampling within groups/strata Simple Random Sampling Using 2 variables to stratify Non-stratified sampling methods", " Other stratification methods The second method we outline is using k means stratification for consequent stratified sampling. K-means Here is a standard way to apply unsupervised K-means and allocate each ALS cell to a cluster. ### K MEANS ### set.seed(123) # Extract values for Kmeans v &lt;- rast_dt_wall ## determine index of each cell so to map values correctly without NA&#39;s idx &lt;- 1:ncell(wall_poly) idx &lt;- idx[-unique(which(is.na(v), arr.ind=TRUE)[,1])] ############################ #### K MEANS CLUSTERING #### ############################ #### DETERMINE OPTIMAL NUMBER OF CLUSTERS????? #Apply K means - center and scale values and remove NA&#39;s E &lt;- kmeans(na.omit(scale(v,center = T,scale=T)), 4) clusters &lt;- E$cluster # create empty raster from original candidate dimentions r.empty &lt;- wall_poly[[1]] r.empty[] &lt;- NA # add cluster values to corresponding index and re-name r.empty[idx] &lt;- clusters names(r.empty) &lt;- c(&quot;clusters&quot;) #combine clusters with original structural metrics wall_poly[[14]] &lt;- r.empty pal &lt;- brewer.pal(n = 4, name = &quot;Spectral&quot;) plot(wall_poly$clusters,col=pal) How many clusters? This is a loaded questions. How many clusters to choose can depends on many reasons, and there are many ways to determine how many to use. A challenge with the K-means method is being objective about the number of strata to use and where to partition the data. I present a number of examples below. (Papa et al. 2020) used k-means clustering and outlined that they used the Elbow method to determine the number of clusters. #Elbow Method for finding the optimal number of clusters set.seed(123) # data &lt;- as.data.frame(na.omit(scale(v))) # Elbow method fviz_nbclust(as.data.frame(na.omit(scale(v))), kmeans, method = &quot;wss&quot;) + geom_vline(xintercept = 4, linetype = 2) + # add line for better visualisation labs(subtitle = &quot;Elbow method&quot;) # add subtitle Based on the figure above we see that we can choose 4 clusters based on the elbow method. There are many methods that can be used. We see quickly that its not entirely cut and dry how to choose cluster numbers, but we have methods to decide that. This is likely something that the user will need to give input on. The NbClust() function from the (Charrad et al. 2015) package is an additional method, though it takes a long time to run (especially on large datasets). This function: provides 30 indices for determining the number of clusters and proposes to user the best clustering scheme from the different results obtained by varying all combinations of number of clusters, distance measures, and clustering methods. nb &lt;- NbClust(data, diss=NULL, distance = &quot;euclidean&quot;, min.nc=2, max.nc=12, method = &quot;kmeans&quot;, index = &quot;all&quot;, alphaBeale = 0.1) Optimum stratum boundaries A method to address this is using optimal break methods like that of strata.data() from the (Reddy and Khan 2019) package. This method takes the population of candidate cells v and a fixed sample size 100 to compute the optimum stratum boundaries (OSB) for a given number of strata. Along with OSB it also provides the optimum sample sizes within strata directly from the data. In this example we specify that we would like data to be split into 4 strata and we iterate splits on the avg cov and p99 variables. Some studies like (Maltamo et al. 2011) and (Hawbaker et al. 2009) used multiple metrics to split on, while others like used only 1. #prepare data o_val &lt;- v %&gt;% na.omit() %&gt;% as.data.frame() ########################################## ####### OPTIMAL CLUSTERING BREAKS ######## ########################################## ##### Sampling within clusters OSB &lt;- list() for(i in c(1,2,10)){ #1 - average height #2 - covariance #10 - p99 height res &lt;- strata.data(o_val[,i], h = 4, n=100) k &lt;- names(o_val[1]) breaks &lt;- data.frame(res$OSB) colnames(breaks)=k OSB[[i]] &lt;- breaks } breaks &lt;- cbind(as.data.frame(OSB[[1]]),as.data.frame(OSB[[2]]),as.data.frame(OSB[[10]])) names(breaks) &lt;- c(&quot;avg&quot;,&quot;cov&quot;,&quot;p99&quot;) saveRDS(breaks,&quot;dat/breaks.RDS&quot;) ggplot(o_val,aes(avg))+ geom_histogram()+ geom_vline(xintercept = breaks$avg, linetype = &quot;dashed&quot;) Sampling within groups/strata Once OSB are defined we can associate the groups with the data themselves and perform statistical tests to determine wherther groups are significantly difference from one another. (Papa et al. 2020) used ANOVA and Tukey post-hoc tests. First, we can change the resulting group labels from cut() to more understandable characters. #define groups within splitting variable split_osb &lt;- c(-Inf,breaks$avg[1:3],Inf) grps &lt;- cut(o_val$avg,split_osb) o_val$groups &lt;- as.factor(grps) #convert groups to A | B | C | D o_val &lt;- o_val %&gt;% mutate(groups = case_when(groups == &quot;(-Inf,5.1]&quot;~&quot;A&quot;, groups == &quot;(5.1,9.33]&quot;~&quot;B&quot;, groups == &quot;(9.33,14.4]&quot;~&quot;C&quot;, groups == &quot;(14.4, Inf]&quot;~&quot;D&quot;)) # Statistical testing summary(mod1 &lt;- aov(avg~groups,data=o_val)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## groups 3 1290629 430210 266539 &lt;2e-16 *** ## Residuals 87230 140794 2 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 o_val %&gt;% ggplot(aes(groups,avg,fill=groups)) + stat_eye() Once these strata are established we can begin to test a variety of sampling mechanisms. Simple Random Sampling ## Without road masking ns &lt;- 200 #total number of desired samples samples &lt;- o_val %&gt;% group_by(groups) %&gt;% summarize(n= n()) %&gt;% mutate(freq = n / sum(n), plot_tot = as.integer(freq*ns)) df_sample &lt;- o_val %&gt;% group_by(groups) %&gt;% arrange(groups) %&gt;% nest() %&gt;% ungroup() %&gt;% mutate(n = samples$plot_tot) %&gt;% mutate(samp = map2(data, n, sample_n)) %&gt;% dplyr::select(-data) %&gt;% unnest(samp) # sample summary statistics df_sample %&gt;% group_by(groups) %&gt;% dplyr::select(avg,groups) %&gt;% summarise( n = n(), mean = mean(avg), sd = sd(avg) ) ## # A tibble: 4 x 4 ## groups n mean sd ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 41 3.34 1.00 ## 2 B 60 7.20 1.12 ## 3 C 79 11.7 1.39 ## 4 D 18 15.9 0.883 #population summary statistics o_val %&gt;% group_by(groups) %&gt;% dplyr::select(avg,groups) %&gt;% summarise( n = n(), mean = mean(avg), sd = sd(avg) ) ## # A tibble: 4 x 4 ## groups n mean sd ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 17933 3.30 1.05 ## 2 B 26395 7.36 1.23 ## 3 C 34626 11.8 1.41 ## 4 D 8280 15.7 1.23 coords &lt;- st_as_sf(df_sample, coords = c(&quot;x&quot;,&quot;y&quot;)) plot(wall_poly$avg) plot(st_geometry(coords),add=T) ggplot(mapping = aes(x=avg,y=std))+ geom_point(data = o_val, aes(color = groups))+ geom_point(data = df_sample, pch=21, fill = &quot;red&quot;,colour=&quot;black&quot;)+ theme_bw() + ylim(0,15) + coord_equal() + theme(panel.grid = element_blank(), legend.position = &quot;right&quot;) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## groups 3 532594 177531 117528 &lt;2e-16 *** ## Residuals 29898 45162 2 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 SRS 2 Variable Road Masked ns &lt;- 200 #total number of desired samples samples &lt;- o_val %&gt;% group_by(groups) %&gt;% summarize(n= n()) %&gt;% mutate(freq = n / sum(n), plot_tot = as.integer(freq*ns)) df_sample &lt;- o_val %&gt;% group_by(groups) %&gt;% arrange(groups) %&gt;% nest() %&gt;% ungroup() %&gt;% mutate(n = samples$plot_tot) %&gt;% mutate(samp = map2(data, n, sample_n)) %&gt;% dplyr::select(-data) %&gt;% unnest(samp) # sample summary statistics df_sample %&gt;% group_by(groups) %&gt;% dplyr::select(avg,groups) %&gt;% summarise( n = n(), mean = mean(avg), sd = sd(avg) ) ## # A tibble: 4 x 4 ## groups n mean sd ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 61 2.88 0.875 ## 2 B 50 6.96 1.20 ## 3 C 69 11.8 1.44 ## 4 D 18 15.6 0.961 #population summary statistics o_val %&gt;% group_by(groups) %&gt;% dplyr::select(avg,groups) %&gt;% summarise( n = n(), mean = mean(avg), sd = sd(avg) ) ## # A tibble: 4 x 4 ## groups n mean sd ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 9126 3.15 0.992 ## 2 B 7590 7.29 1.23 ## 3 C 10404 11.9 1.41 ## 4 D 2782 15.7 1.21 Visualizing the samples coords &lt;- st_as_sf(df_sample, coords = c(&quot;x&quot;,&quot;y&quot;)) plot(wall_poly_roads$avg) plot(st_geometry(coords),add=T) ggplot(mapping = aes(x=avg,y=std))+ geom_point(data = o_val, aes(color = groups))+ geom_point(data = df_sample, pch=21, fill = &quot;red&quot;,colour=&quot;black&quot;)+ theme_bw() + ylim(0,15) + coord_equal() + theme(panel.grid = element_blank(), legend.position = &quot;right&quot;) Using 2 variables to stratify ## Without road masking # Extract values for Kmeans v &lt;- rast_dt_wall o_val &lt;- v %&gt;% na.omit() %&gt;% as.data.frame() #define groups within splitting variable split_osb &lt;- c(-Inf,breaks$avg[1:3],Inf) grps &lt;- cut(o_val$avg,split_osb) o_val$groups &lt;- as.character(grps) #convert groups to A | B | C | D o_val &lt;- o_val %&gt;% mutate(groups = case_when(groups == &quot;(-Inf,5.1]&quot;~1, groups == &quot;(5.1,9.33]&quot;~2, groups == &quot;(9.33,14.4]&quot;~3, groups == &quot;(14.4, Inf]&quot;~4)) o_val &lt;- o_val %&gt;% group_by(groups) %&gt;% mutate(groups2 = ntile(std,3)) %&gt;% unite(&quot;groups_m&quot;,groups,groups2) %&gt;% mutate(id = group_indices(., groups_m)) ns &lt;- 200 #total number of desired samples samples &lt;- o_val %&gt;% group_by(groups_m) %&gt;% summarize(n= n()) %&gt;% mutate(freq = n / sum(n),plot_tot = as.integer(freq*ns)) samp &lt;- o_val %&gt;% group_by(groups_m) %&gt;% arrange(groups_m) %&gt;% nest() %&gt;% ungroup() %&gt;% mutate(n = samples$plot_tot) %&gt;% mutate(samp = map2(data, n, sample_n)) %&gt;% dplyr::select(-data) %&gt;% unnest(samp) coords &lt;- st_as_sf(samp, coords = c(&quot;x&quot;,&quot;y&quot;)) plot(wall_poly$avg) plot(st_geometry(coords),add=T) p &lt;- ggplot(mapping = aes(x=avg,y=std))+ geom_point(data = o_val, aes(color = groups_m))+ geom_point(data = samp, pch=21, fill = &quot;red&quot;,colour=&quot;black&quot;)+ theme_bw() + coord_equal() + theme(panel.grid = element_blank(), legend.position = &quot;right&quot;) coordsgrps &lt;- o_val %&gt;% group_by(groups_m) %&gt;% arrange(groups_m) %&gt;% nest() %&gt;% ungroup() p + geom_rect(data=coordsgrps$data[[1]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[2]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[3]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[4]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[5]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[6]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[7]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[8]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[9]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[10]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[11]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[12]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA) 2 Variable Road Masked ## With road masking # Extract values for Kmeans v &lt;- rast_dt o_val &lt;- v %&gt;% na.omit() %&gt;% as.data.frame() #define groups within splitting variable split_osb &lt;- c(-Inf,breaks$avg[1:3],Inf) grps &lt;- cut(o_val$avg,split_osb) o_val$groups &lt;- as.character(grps) #convert groups to A | B | C | D o_val &lt;- o_val %&gt;% mutate(groups = case_when(groups == &quot;(-Inf,5.1]&quot;~1, groups == &quot;(5.1,9.33]&quot;~2, groups == &quot;(9.33,14.4]&quot;~3, groups == &quot;(14.4, Inf]&quot;~4)) o_val &lt;- o_val %&gt;% group_by(groups) %&gt;% mutate(groups2 = ntile(std,3)) %&gt;% unite(&quot;groups_m&quot;,groups,groups2) %&gt;% mutate(id = group_indices(., groups_m)) ns &lt;- 200 #total number of desired samples samples &lt;- o_val %&gt;% group_by(groups_m) %&gt;% summarize(n= n()) %&gt;% mutate(freq = n / sum(n),plot_tot = as.integer(freq*ns)) samp &lt;- o_val %&gt;% group_by(groups_m) %&gt;% arrange(groups_m) %&gt;% nest() %&gt;% ungroup() %&gt;% mutate(n = samples$plot_tot) %&gt;% mutate(samp = map2(data, n, sample_n)) %&gt;% dplyr::select(-data) %&gt;% unnest(samp) coords &lt;- st_as_sf(samp, coords = c(&quot;x&quot;,&quot;y&quot;)) plot(wall_poly_roads$avg) plot(st_geometry(coords),add=T) p &lt;- ggplot(mapping = aes(x=avg,y=std))+ geom_point(data = o_val, aes(color = groups_m))+ geom_point(data = samp, pch=21, fill = &quot;red&quot;,colour=&quot;black&quot;)+ theme_bw() + coord_equal() + theme(panel.grid = element_blank(), legend.position = &quot;right&quot;) coordsgrps &lt;- o_val %&gt;% group_by(groups_m) %&gt;% arrange(groups_m) %&gt;% nest() %&gt;% ungroup() p + geom_rect(data=coordsgrps$data[[1]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[2]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[3]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[4]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[5]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[6]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[7]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[8]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[9]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[10]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[11]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[12]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA) # Convert dataframe back to raster to show classes spg &lt;- o_val coordinates(spg) &lt;- ~ x + y # coerce to SpatialPixelsDataFrame gridded(spg) &lt;- TRUE # coerce to raster rasterDF &lt;- raster(spg, &quot;id&quot;) #plot pal &lt;- brewer.pal(n = 12, name = &quot;Spectral&quot;) ## Warning in brewer.pal(n = 12, name = &quot;Spectral&quot;): n too large, allowed maximum for palette Spectral is 11 ## Returning the palette you asked for with that many colors plot(rasterDF,col=pal) plot(st_geometry(coords),add=T) Non-stratified sampling methods Some sampling methods such as the BalancedSampling approaches and Latin Hypercube Sampling do not require prior stratification. In order to sample these we present these methods on only candidate samples within the road buffer subsets. #prepare data set.seed(123) # Extract values for Kmeans v &lt;- rast_dt o_val &lt;- v %&gt;% na.omit() %&gt;% as.data.frame() Balanced Sampling (Grafström and Ringvall 2013) described the (Grafström and Lisic 2019) package, which implements a number of sampling methods that balance samples spatially and within auxilary variable space. The spatial balance is important given that we library(BalancedSampling) # Example 1 set.seed(123); N = nrow(o_val); # population size n = 200; # sample size p = rep(n/N,N);# inclusion probabilities X = as.matrix(o_val) # matrix of auxiliary variables s1 = lpm1(p,X); # select sample plot(X[,3],X[,5]); # plot population points(X[s1,3],X[s1,5], pch=19,col=&quot;red&quot;); # plot sample plot(X[,1],X[,2]); # plot population points(X[s1,1],X[s1,2], pch=19,col=&quot;red&quot;); # plot sample s2 = lpm2(p,X); # select sample plot(X[,3],X[,5]); # plot population points(X[s2,3],X[s2,5], pch=19,col=&quot;red&quot;); # plot sample plot(X[,1],X[,2]); # plot population points(X[s2,1],X[s2,2], pch=19,col=&quot;red&quot;); # plot sample s3 = lcube(p,X,cbind(p)); plot(X[,1],X[,2]); # plot population points(X[s3,1],X[s3,2], pch=19,col=&quot;red&quot;); # plot sample plot(X[,3],X[,5]); # plot population points(X[s3,3],X[s3,5], pch=19,col=&quot;red&quot;); # plot sample sampls1 &lt;- as.data.frame(X[s1,]) %&gt;% mutate(alg = &quot;lmp1&quot;) sampls2 &lt;- as.data.frame(X[s2,]) %&gt;% mutate(alg = &quot;lmp2&quot;) sampls3 &lt;- as.data.frame(X[s3,]) %&gt;% mutate(alg = &quot;lcube&quot;) samps &lt;- rbind(sampls1,sampls2,sampls3) samps %&gt;% group_by(alg) %&gt;% dplyr::select(avg) %&gt;% summarise( n = n(), mean = mean(avg), sd = sd(avg) ) ## # A tibble: 3 x 4 ## alg n mean sd ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 lcube 200 8.26 4.32 ## 2 lmp1 200 8.26 4.51 ## 3 lmp2 200 8.22 4.50 o_val %&gt;% dplyr::select(avg) %&gt;% summarise( n = n(), mean = mean(avg), sd = sd(avg) ) ## n mean sd ## 1 29902 8.396322 4.395713 Latin Hypercube Sampling There are a number of different approaches to take with Latin Hypercube Sampling. The following code was derived from (Malone, Minansy, and Brungard 2019), who present a method to boostrap sample number for hypercube sampling, followed by a few testing options to determine optimal sample numbers. This code was provided by the authors at https://bitbucket.org/brendo1001/clhc_sampling/downloads/ which i have manipulated to work with our dataset. #################################################################### # Data analysis for the population data #Principal components of the population (the is for tests 1 and 2) pca1 = prcomp(df[,3:6],scale=TRUE, center=TRUE) scores_pca1 = as.data.frame(pca1$x) screeplot(pca1) ## plot the variances explained by each component biplot(pca1) summary(pca1) # retreive the loadings pca1.load&lt;- matrix(NA,ncol=4,nrow=4 ) for (i in 1:4){ pca1.load[i,]&lt;- as.matrix(t(pca1$rotation[i,]))} #Quantiles of the population (this is for test 3) # Number of bins nb&lt;- 25 #quantile matrix (of the covariate data) q.mat&lt;- matrix(NA, nrow=(nb+1), ncol= 4) j=1 for (i in 3:ncol(df)){ #note the index start here #get a quantile matrix together of the covariates ran1&lt;- max(df[,i]) - min(df[,i]) step1&lt;- ran1/nb q.mat[,j]&lt;- seq(min(df[,i]), to = max(df[,i]), by =step1) j&lt;- j+1} q.mat #covariate data hypercube (this is for test 4) ## This takes a while to do so only do it once if you can cov.mat&lt;- matrix(1, nrow=nb, ncol=4) for (i in 1:nrow(df)){ # the number of pixels cntj&lt;- 1 for (j in 3:ncol(df)){ #for each column dd&lt;- df[i,j] for (k in 1:nb){ #for each quantile kl&lt;- q.mat[k, cntj] ku&lt;- q.mat[k+1, cntj] if (dd &gt;= kl &amp; dd &lt;= ku){cov.mat[k, cntj]&lt;- cov.mat[k, cntj] + 1} } cntj&lt;- cntj+1 } } cov.mat #################################################################### roads &lt;- st_read(system.file(&quot;extdata/roads&quot;,&quot;roads.shp&quot;, package = &quot;RMFinventory&quot;)) roads &lt;- st_transform(roads, wall_crs@projargs) ####################################################################### #How many samples do we need? #beginning of algorithm #initial settings cseq&lt;- seq(10,500,10) # cLHC sample size its&lt;-10 # number internal iterations with each sample size number mat.seq&lt;- matrix(NA,ncol=8,nrow=length(cseq)) #empty matix for outputs for (w in 1:length(cseq)){ # for every sample number configuration.... s.size=cseq[w] # sample size mat.f&lt;- matrix(NA,ncol=8,nrow=its ) # placement for iteration outputs #internal loop for (j in 1:its){ #Note that this takes quite a while to run to completion repeat{ ss &lt;- clhs(df[,3:6], size = s.size, progress = T, iter = 100) # Do a conditioned latin hypercube sample s.df&lt;- df[ss,] if (sum(duplicated(s.df) | duplicated(s.df[nrow(s.df):1, ])[nrow(s.df):1]) &lt; 2) {break}} #principal component of sample pca.s = prcomp(s.df[,3:6],scale=TRUE, center=TRUE) scores_pca1 = as.data.frame(pca.s$x) # plot the first 2 principal components and convex hull rand.tr&lt;-tri.mesh(scores_pca1[,1],scores_pca1[,2]) rand.ch&lt;-convex.hull(rand.tr, plot.it=F) #convex hull pr_poly = cbind(x=c(rand.ch$x),y=c(rand.ch$y)) # save the convext hull vertices #plot(scores_pca1[,1], scores_pca1[,2], xlab=&quot;PCA 1&quot;, ylab=&quot;PCA 2&quot;, xlim=c(min(scores_pca1[,1:2]), max(scores_pca1[,1:2])),ylim=c(min(scores_pca1[,1:2]), max(scores_pca1[,1:2]))) #lines(c(rand.ch$x,rand.ch$x[1]), c(rand.ch$y,rand.ch$y[1]),col=&quot;red&quot;,lwd=1) # draw the convex hull(domain of prediction) #### First Test: #points in polygons routine # PCA prjection PCA_projection&lt;- predict(pca.s, df[,3:6]) # Project population onto sample PC newScores = cbind(x=PCA_projection[,1],y=PCA_projection[,2]) # PC scores of projected population #plot the polygon and all points to be checked #plot(newScores,xlab=&quot;PCA 1&quot;, ylab=&quot;PCA 2&quot;, xlim=c(min(newScores[,1:2]), max(newScores[,1:2])),ylim=c(min(newScores[,1:2]), max(newScores[,1:2])),col=&#39;black&#39;, main=&#39;convex hull of ss&#39;) # polygon(pr_poly,col=&#39;#99999990&#39;) #create check which points fall within the polygon specMatch = pip2d(pr_poly,newScores) specMatch = specMatch &gt; 0 mat.f[j,7]&lt;- sum(specMatch)/length(specMatch)*100 # propertion of new spectra the fall within the convex hull # points(specMatch[which(specMatch==0),1:2],pch=&#39;X&#39;, col=&#39;red&#39;) ##END points in polygons## #### Second Test: #similarity of the 2 matrices (PCA Similarity Factor; Krzanowski (1979)) # retreive the loadings for the samples pca.s.load&lt;- matrix(NA,ncol=4,nrow=4 ) for (i in 1:4){ pca.s.load[i,]&lt;- as.matrix(t(pca.s$rotation[i,])) } # Perfrom the Krznowski 1979 calculation ps1&lt;- pca1.load[,1:2] ps2&lt;- pca.s.load[,1:2] ps1.t&lt;- t(ps1) #transpose ps2.t&lt;- t(ps2) #transpose S&lt;- ps1.t %*% ps2 %*% ps2.t %*% ps1 mat.f[j,1]&lt;-sum(diag(S))/2 ## Third Test: #comparison of quantiles df.q2.1&lt;-quantile(s.df[,3], probs = seq(0, 1, 0.25),names = F, type = 7) df.q1.1&lt;-quantile(df[,3], probs = seq(0, 1, 0.25),names = F, type = 7) mat.f[j,2]&lt;-sqrt((df.q1.1[1]-df.q2.1[1])^2 + (df.q1.1[2]-df.q2.1[2])^2 + (df.q1.1[3]-df.q2.1[3])^2 + (df.q1.1[4]-df.q2.1[4])^2 ) df.q2.2&lt;-quantile(s.df[,4], probs = seq(0, 1, 0.25),names = F, type = 7) df.q1.2&lt;-quantile(df[,4], probs = seq(0, 1, 0.25),names = F, type = 7) mat.f[j,3]&lt;-sqrt((df.q1.2[1]-df.q2.2[1])^2 + (df.q1.2[2]-df.q2.2[2])^2 + (df.q1.2[3]-df.q2.2[3])^2 + (df.q1.2[4]-df.q2.2[4])^2 ) df.q2.3&lt;-quantile(s.df[,5], probs = seq(0, 1, 0.25),names = F, type = 7) df.q1.3&lt;-quantile(df[,5], probs = seq(0, 1, 0.25),names = F, type = 7) mat.f[j,4]&lt;-sqrt((df.q1.3[1]-df.q2.3[1])^2 + (df.q1.3[2]-df.q2.3[2])^2 + (df.q1.3[3]-df.q2.3[3])^2 + (df.q1.3[4]-df.q2.3[4])^2 ) df.q2.4&lt;-quantile(s.df[,6], probs = seq(0, 1, 0.25),names = F, type = 7) df.q1.4&lt;-quantile(df[,6], probs = seq(0, 1, 0.25),names = F, type = 7) mat.f[j,5]&lt;-sqrt((df.q1.4[1]-df.q2.4[1])^2 + (df.q1.4[2]-df.q2.4[2])^2 + (df.q1.4[3]-df.q2.4[3])^2 + (df.q1.4[4]-df.q2.4[4])^2 ) mat.f[j,6]&lt;-mean(mat.f[j,2:5]) # take the mean distance ## Fourth test: Kullback-Leibler (KL) divergence ####Compare whole study area covariate space with the selected sample #sample data hypercube (essentially the same script as for the grid data but just doing it on the sample data) h.mat&lt;- matrix(1, nrow=nb, ncol=4) for (ii in 1:nrow(s.df)){ # the number of observations cntj&lt;- 1 for (jj in 3:ncol(s.df)){ #for each column dd&lt;- s.df[ii,jj] for (kk in 1:nb){ #for each quantile kl&lt;- q.mat[kk, cntj] ku&lt;- q.mat[kk+1, cntj] if (dd &gt;= kl &amp; dd &lt;= ku){h.mat[kk, cntj]&lt;- h.mat[kk, cntj] + 1} } cntj&lt;- cntj+1}} #h.mat #Kullback-Leibler (KL) divergence klo.1&lt;- KL.empirical(c(cov.mat[,1]), c(h.mat[,1])) #1 klo.2&lt;- KL.empirical(c(cov.mat[,2]), c(h.mat[,2])) #2 klo.3&lt;- KL.empirical(c(cov.mat[,3]), c(h.mat[,3])) #3 klo.4&lt;- KL.empirical(c(cov.mat[,4]), c(h.mat[,4])) #4 klo&lt;- mean(c(klo.1, klo.2,klo.3,klo.4)) mat.f[j,8]&lt;- klo # value of 0 means no divergence } #arrange outputs mat.seq[w,1]&lt;-mean(mat.f[,6]) mat.seq[w,2]&lt;-sd(mat.f[,6]) mat.seq[w,3]&lt;-min(mat.f[,1]) mat.seq[w,4]&lt;-max(mat.f[,1]) mat.seq[w,5]&lt;-mean(mat.f[,7]) mat.seq[w,6]&lt;-sd(mat.f[,7]) mat.seq[w,7]&lt;-mean(mat.f[,8]) mat.seq[w,8]&lt;-sd(mat.f[,8]) } ## END of LOOP dat.seq&lt;- as.data.frame(cbind(cseq,mat.seq)) names(dat.seq)&lt;- c(&quot;samp_nos&quot;, &quot;mean_dist&quot;,&quot;sd_dist&quot;, &quot;min_S&quot;, &quot;max_S&quot;, &quot;mean_PIP&quot;,&quot;sd_PIP&quot;, &quot;mean_KL&quot;,&quot;sd_KL&quot;) ########################################################## After the loop has finished we are able to plot the ouputs of the bootstrapping to outline how the sample size influenced the deviation of population and sample statistics. ####################################################### #plot some outputs plot(cseq,mat.seq[,1], xlab=&quot;number of samples&quot;, ylab= &quot;similarity between covariates (entire field) with covariates (sample)&quot;,main=&quot;Population and sample similarity&quot;) plot(cseq,mat.seq[,2],xlab=&quot;number of samples&quot;, ylab= &quot;standard deviation similarity between covariates (entire field) with covariates (sample)&quot;,main=&quot;Population and sample similarity (sd)&quot;) plot(cseq,mat.seq[,3]) plot(cseq,mat.seq[,4]) plot(cseq,mat.seq[,5],xlab=&quot;number of samples&quot;, ylab= &quot;percentage of total covariate variance of population account for in sample&quot;,main=&quot;Population and sample similarity&quot;) plot(cseq,mat.seq[,6],xlab=&quot;number of samples&quot;, ylab= &quot;standard deviation of percentage of total covariate variance of population account for in sample&quot;,main=&quot;Population and sample similarity&quot;) plot(cseq,mat.seq[,7],xlab=&quot;number of samples&quot;, ylab= &quot;KL divergence&quot;) plot(cseq,mat.seq[,8],xlab=&quot;number of samples&quot;, ylab= &quot;standard deviation of percentage of total covariate variance of population account for in sample&quot;,main=&quot;Population and sample similarity&quot;) ########################################################## ########################################################## # make an exponetial decay function (of the KL divergence) x&lt;- dat.seq$samp_nos y = 1- (dat.seq$mean_PIP-min(dat.seq$mean_PIP))/(max(dat.seq$mean_PIP)-min(dat.seq$mean_PIP)) #PIP #Parametise Exponential decay function plot(x, y, xlab=&quot;sample number&quot;, ylab= &quot;1 - PC similarity&quot;) # Initial plot of the data start &lt;- list() # Initialize an empty list for the starting values #fit 1 manipulate( { plot(x, y) k &lt;- kk; b0 &lt;- b00; b1 &lt;- b10 curve(k*exp(-b1*x) + b0, add=TRUE) start &lt;&lt;- list(k=k, b0=b0, b1=b1) }, kk=slider(0, 5, step = 0.01, initial = 2), b10=slider(0, 1, step = 0.000001, initial = 0.01), b00=slider(0,1 , step=0.000001,initial= 0.01)) fit1 &lt;- nls(y ~ k*exp(-b1*x) + b0, start = start) summary(fit1) lines(x, fitted(fit1), col=&quot;red&quot;) After determining the disparity between samples and population we can optimize the number of samples we use based on the cumulative frequency distribution. We determined that the optimum number of samples was 119. #Apply fit xx&lt;- seq(1, 500,1) lines(xx, predict(fit1,list(x=xx))) jj&lt;- predict(fit1,list(x=xx)) normalized = 1- (jj-min(jj))/(max(jj)-min(jj)) x&lt;- xx y&lt;- normalized plot(x, y, xlab=&quot;sample number&quot;, ylab= &quot;normalised PIP&quot;, type=&quot;l&quot;, lwd=2) # Initial plot of the data x1&lt;- c(-1, 500) y1&lt;- c(0.95, 0.95) lines(x1,y1, lwd=2, col=&quot;red&quot;) num &lt;- data.frame(xx,normalized) %&gt;% filter(abs(normalized - 0.95) == min(abs(normalized - 0.95))) %&gt;% dplyr::select(xx) %&gt;% pull() paste0(&quot;your optimum sample size is: &quot;,num) x2&lt;- c(num, num); y2&lt;- c(0, 1) lines(x2,y2, lwd=2, col=&quot;red&quot;) ############################################################################# ##END set.seed(2021) df &lt;- rast_dt %&gt;% dplyr::select(x,y,avg,std,cov,p99) %&gt;% na.omit() %&gt;% as.data.frame() ss &lt;- clhs(df[,3:6], size = 143, progress = T, iter = 10) # Do a conditioned latin hypercube sample ## | | | 0% | |======== | 11% | |================ | 22% | |======================= | 33% | |=============================== | 44% | |======================================= | 56% | |=============================================== | 67% | |====================================================== | 78% | |============================================================== | 89% | |======================================================================| 100% s.df&lt;- df[ss,] coords &lt;- st_as_sf(s.df, coords = c(&quot;x&quot;,&quot;y&quot;)) plot(wall_poly_roads$avg) plot(st_geometry(coords),add=T) This may look well and good.. But its important to look at WHERE the samples are being located to see if the algorithm was effective. See the image below showing that that we likely need to introduce a spatial aspect to the sampling. Not really sure how to do this just yet References "]]

[["index.html", "Structurally Guided Sampling Overview", " Structurally Guided Sampling Tristan Goodbody 2021-03-22 Overview This document is a brief introduction to the structurally guided sampling project being undertaken by the IRSS Lab at the UBC Faculty fo Forestry. Herein we provide details related to: Literature review about SGS Overview of how data has been prepared Work done in the RMFInventory package using principle components Potential methods to apply using stratification and sampling methods Points to discuss for how the project should evolve "],["SGS.html", "Structurally Guided Sampling Review Stratification approaches", " Structurally Guided Sampling Review (McRoberts, Gobakken, and Næsset 2012) outline the essence of structurally guided sampling: Stratified estimation is a statistical technique that can be used to increase the precision of estimates without increasing sample sizes. The essence of stratified estimation is to aggregate observations of the response variable into groups or strata that are more homogeneous than the population as a whole. The population mean and its variance are estimated as weighted means ofwithin-strata means and variances where the weights are based on strata sizes. If the stratification is accomplished prior to sampling and the within-stratum variances are known or can be easily estimated, then greater precision may be achieved by selecting within-strata sampling intensities to be proportional to within-strata variances (Cochran, 1977). Following literature review I present a number of implementations that have been used to guide the stratification of ALS-derived structural variables for locating representative sample plots. Studies including (Leiterer et al. 2015) and (Kane et al. 2010) suggest that the information in the ALS data may be condensed to a few metrics. Many of the studies reviewed seem to adopt this by selecting a low number of metrics which they use to stratify the landscape and consequently sample within.(Fedrigo et al. 2018) further outlined: Lidar metrics may be useful for forest stand classification if they differentiate classes of interest (Wulder et al., 2008). The separation of the data into vertical layers allows metrics to be derived for specific forest strata, which may increase the discrimination of multi-strata stand types. The selection of breaks that define these strata can be based on field knowledge or identified statistically from lidar profiles (Lefsky et al., 1999; Zhang et al., 2011). An additional note of interest is that a great deal of research in the field of digital soil mapping can be applied to structurally guided sampling. Studies including (Carré, McBratney, and Minasny 2007), (Ma et al. 2020), and (Malone, Minansy, and Brungard 2019) were invlauable for understanding how to use auxillary variables to determine effective sampling protocols - especially in the context of Lahin Hypercube Sampling. Stratification approaches (Hawbaker et al. 2009) Used ALS mean and SD to stratify. First they separated forest types then 10 strata were derived suing mean height. Within each stratum, 3 additional sub-strata were created using SD. This produced a total of 30 stratifications where one coniferous forest and two deciduous forest locations randomly selected. They compared samples taken from strata with a random sample finding that: &gt;Prediction errors from models constructed using a random sample were up to 68% larger than those from the models built with a stratified sample. The stratified sample included a greater range of variability than the random sample. (Dalponte et al. 2011) They analyzed 2 aspects of ground reference data collection. The positioning error of the ground plots The optimal number of training plots. They had a large collection of samples &gt;700 and systematically analyzed how the reduction of sample numbers influenced accuracy of attribute estimates. They tested the effect of error in estimate related to distance the center point of the plot was moved. The decrease was at a minimum when the displacement was less than 5 m, while it was considerable over 10 m. They did random downsampling of the sample plot number and stratified downsampling of the sample plot number. They used the methods from (Hawbaker et al. 2009) and (Maltamo et al. 2011) for stratified downsampling. Comparing the results obtained with the proposed stratified sampling method and the other three methods considered (see Table 3), one can see that the proposed stratified sampling method was the one that provided the best results for both sets. The new protocol allowed us to obtain promising results for the considered dataset: using only 53 training plots, instead of 534 in the original dataset, we obtained the same results for the validation set. (Maltamo et al. 2011) Used divided P90 into 12 even parts. A near equal number of sample plots were systematically chosen within each of these 12 parts according to the range of VEG (ratio of vegetation points and ground points). To representatively sample the 12 parts and to cover the extremes, selection intensity was higher in the p90 range range with the lowest number of previously established plots. The use of ALS data as a priori information provided the most accurate results. They also provide the an important statement: One should keep in mind that in real world applications ,some given ALS variables will have to be used for selection of plots regardless of which and how many biophysical properties the inventory aims at estimating. Thus, some ALS variables will work better for certain biophysical properties than for others. An appropriate way of choosing suitable ALS variables could be to select them according to the relative importance of the biophysical properties to be estimated.for volume and number of stems. plots. REduced total sample size incrementally. (Junttila et al. 2013) Used a number of approaches including a space filling design, which attempts to sample uniformly across a feature space. The Maximin design attempts to cover a feature space as uniformly as possible given a fixed number of sample points. It maximizes the minimum distance between plots. The results indicate that choosing the plots in a way that ensures ample coverage of both spatial and feature space variability improves the performance of the corresponding models, and that adequate coverage of the variability in the feature space is the most important condition that should be met by the set of plots collected. (Valbuena et al. 2013) Used the Cover metric to privde a idea of the density of each forest area. The justified that cover influenced the relation between other metrics and the forest response and, therefore, it was included in all the models computed. (Grafström and Ringvall 2013) Presents the (Grafström and Lisic 2019) package. and does comparisons between Simple random sampling (SRS) and: Local pivotal method with x and y coordinates (LPM-xy) to make sure the sample is well spread geographically. The Euclidean distance is used without standardization because the spatial coordinates are on the same scale. Local pivotal method (LPM) XY and four selected ALS variables as auxiliary variables. The samples are well spread geographically and well spread in the four ALS variables. The distance function (eq. (3)) is used. Cube method (CM) The samples are balanced on the four ALS variables. The x and y coordinates are not used for this method, because it does not make sense to balance on geographical coordinates. Local cube method (LCM) The samples are balanced on the four ALS variables and the x and y coordinates are used to make sure the sample is well spread geographically. The Euclidean distance is used in the geographical space. They found that spreading the sample well in geographical space is important. Spreading the sample well in all auxiliary variables (LPM) and using the HT estimator was in all cases a more efficient strategy than SRS followed by a calibration estimator. Spreading the sample well is more efficient than balancing (or SRS followed by calibration) if the relationship between the target and the auxiliary variables is nonlinear, which is why it performs better (see Tables 2 and 3). The final order of the designs combined with the calibration estimator was (from best to worst) LCM, LPM, LPM-xy, CM, and SRS. We get the best results when using the information both in the design and the estimator. Hence, we can conclude that using auxiliary information in the design is very important. \" All three designs, LCM, LPM, and CM, had very good design effects. The reduction of the variance of the HT estimator was for the main target variable volume up to 75% compared with SRS. (Grafström, Saarela, and Ene 2014) Continuation of previous study highlighting value of sampling methods. A possible drawback of the LPM compared with more simple methods such as systematic random sampling is that sample se- lection is computationally intensive for large populations. This is a problem because the population size (number of pixels) tends to be rather large in forest applications with auxiliary data from, e.g., airborne LiDAR, making it practically impossible to apply the LPM as it is. To deal with this problem, we introduce a new rapid suboptimal implementation ofthe LPM that can be applied to large populations. With the LPM, there is no stratification or other difficult design choices; we only need to define distance. Modelling is not really necessary, which means no complicated formulas. Yet this is a very competitive strategy in many cases. It has the advantage of being intuitive and simple, which should not be underrated. It also allows us to keep the traditional design-based approach, while still taking advantage of powerful auxiliary information. With the LPM, there is no stratification or other difficult design choices; we only need to define distance. Modelling is not really necessary, which means no complicated formulas. Yet this is a very competitive strategy in many cases. It has the advantage of being intuitive and simple, which should not be underrated. It also allows us to keep the traditional design-based approach, while still taking advantage of powerful auxiliary information. (Niemi and Vauhkonen 2016) Used ALS textural CHM metrics (HIST,PATCH,GLCM,LBP) for pre-stratification of the inventory area. They tested the strength of the relationship between metrics and central attributes of forest growing stock (Vol,BA,Diam). It was found that the dispersion of the clusters derived in an unsupervised mode could be used as an indicator for prioritizing the plots to be measured as the sample to form the reference data for the wall-to-wall models. (Valbuena et al. 2017) Used a rule based method to stratify forest areas using Lmoments including Lcv and Lskew. (McRoberts, Chen, and Walters 2017) Modeling inventory attributes with ALS metrics. BA was considered as an integrator of all the response variables and was used as the basis for constructing strata using a lonear model. Predictions of BA were then scaled by the largest BA prediction,multiplied by 100, then assigned 1 of 100 potential strata. Strata were then aggregated these into 4,6,8 strata with approximately equal proportion of study area. Stratifications that are most effective with respect to minimizing variances are based on variables that are closely related to the response variable or variables of interest. When multiple response variables are to be estimated simultaneously, the same stratification must be used for all response variables to ensure compatibility. Results showed they were able to: &gt;Reduce sampling intensity by up to 35% without losing accuracy in the estimators. Relative to the simple random sampling estimators that used no stratification, use of four strata decreased SEs by 29.0%38.1%, depending on the response variable; relative to four strata, use of six strata reduced SEs by 2.3%7.3%; and relative to six strata, use of eight strata reduced SEs by no more than 4% and for two response variables actually increased SEs. From an operational perspective, the results of the study suggest that multivariate, airborne laser scanning-assisted inventories could be fairly easily implemented, subject to availability of the airborne laser scanning data. (Malone, Minansy, and Brungard 2019) They provide an indepth description of how to implement and improve the outputs of Latin Hypercube sampling, providing a coded example of how to incorporate already acquired samples accross the landscape. They specify a method to boostrap sample size followed by the application of multiple methods to determine the optimal sample number to use based on auxillary data. (Papa et al. 2020) Compared simple random sampling to stratified sampling within strata groups. Used hierarchical cluster analysis for formation of strata. Clusters were used for sub-sampling. The number of clusters was established by the Elbow method. Clusters were validated with an ANOVA post-hoc Tukey test. Varying size of metric cells (1, 0.5, 0.25 ha) were tested. The stratified sampling was simulated by using weighting distributed by strata, so that each segment was the same sample size, whereas no weighting was used for the simulation in the case of the simple random sampling. For each level of sampling size (number of plots in the inventory), 1000 iterations were performed, and their mean and standard deviation were summarized in order to produce a graph of relative uncertainty (standard error/global mean) as a function of sample intensity (proportion of total area sampled, in %). (Ma et al. 2020) They compare latin hypercube sampling and feature space coverage sampling for predicting soil classes. They detail specific functions to use for latin hypercube sampling and feature space coverage sampling. Sampling pools of varying sizes are compared to simple random sampling in a number of modeling approaches. Final results indicated that: 1)In both study areas the median overall accuracy with FSCS washigher than those with CLHS and SRS over all sample sizes and crossall three prediction methods. The median overall accuracy with CLHS was only marginally larger than with SRS. 2)There was a significant negative correlation betweenMSSSDandoverall accuracy, whereas no such correlation was found betweenO1 + O3and overall accuracy.The coefficient of variation in overall accuracy among samples se-lected using FSCS was smaller than these using CLHS and SRS at thesame sample sizes. 3)With CLHS the variation in overall accuracy among samples waslarge, so that there is a serious risk that a particular sample mightlead to a low overall accuracy. 4)FSCS-RF is the most accurate combination of sampling and predic-tion for both study areas Results indicated that feature space coverage sampling could be a viable method for locating samples. pre { max-height: 300px; overflow-y: auto; } pre[class] { max-height: 100px; } References "],["dataprep.html", "Data Preparation Load wall-to-wall metrics Integrate forest inventory layers Prepare road buffers for future masking", " Data Preparation To show the sampling methods in action I use the example data from the Romeo Malette forest in Ontario. These data are included in the RMFInventory package. These data were prepared suing the following method: Compile wall-to-wall structural metrics from ALS acquisition Utilize spatial forest inventory layer to isolate regions of interest for sampling Create internal and external road buffer for future access buffering Load wall-to-wall metrics #Load required libraries library(RMFinventory) library(raster) library(sf) ### Raster DATa wall_metrics &lt;- brick(system.file(&quot;extdata&quot;,&quot;wall_metrics_small.tif&quot;, package = &quot;RMFinventory&quot;)) names(wall_metrics) &lt;- c(&quot;avg&quot;, &quot;cov&quot;, &quot;std&quot;,&quot;p10&quot;, &quot;p20&quot;,&quot;p50&quot;,&quot;p70&quot;,&quot;p95&quot;, &quot;p99&quot;,&quot;d0&quot;,&quot;d2&quot;,&quot;d4&quot;,&quot;dns&quot;) #Coordinate system of wall-to-wall raster wall_crs &lt;- raster::crs(wall_metrics) #Plot plot(wall_metrics$avg) Here we see the wall-to-wall coverage of ALS for our study area. We must be cognizant that some of the area within this coverage is not appropriate or desired from an inventory sampling perspective. This requires forest managers or data users to know which areas should be included/excluded for future stratification and/or sampling protocols. Integrate forest inventory layers To filter areas we use the following approach where spatial forest inventory layers are used to mask wall-to-wall metrics to isolate regions for sampling. ### forest polygons #read forest polygon shapefile poly &lt;- st_read(system.file(&quot;extdata/inventory_polygons&quot;,&quot;inventory_polygons.shp&quot;, package = &quot;RMFinventory&quot;)) ## Reading layer `inventory_polygons&#39; from data source `G:\\Programs\\R-4.0.3\\library\\RMFinventory\\extdata\\inventory_polygons\\inventory_polygons.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 632 features and 111 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 431100 ymin: 5337700 xmax: 438560 ymax: 5343240 ## projected CRS: UTM_Zone_17_Northern_Hemisphere #Match CRS of wall-to-wall raster layer poly &lt;- st_transform(poly, wall_crs@projargs) poly_subset &lt;- poly[poly$POLYTYPE == &quot;FOR&quot; &amp; poly$OWNER == 1, ] plot(st_geometry(poly_subset), axes = TRUE, col = &quot;red&quot;) poly_subset &lt;- st_union(poly_subset) plot(st_geometry(poly_subset), axes = TRUE, col = &quot;red&quot;) #Transform to SpatialPolygonDataFrame object for compatibility with the raster package poly_subset &lt;- sf::as_Spatial(poly_subset) #Mask wall-to-wall layer wall_poly &lt;- raster::mask(wall_metrics, mask = poly_subset) plot(wall_poly$avg) ## Convert raster into data table for tabular representation of candidates rast_dt_wall &lt;- as.data.table.raster(wall_poly,xy=T) We have masked regions based on ownership and forest type, leaving forested areas that are suitable for stratification and sampling. Prepare road buffers for future masking Access is an important consideration for forest development, and in the case of inventory sampling could be leveraged to reduce operational costs and improve efficiency of measurement campaigns. For this reason we also present a method to use available road network layers to generate internal and external road buffers to isolate candidate ALS pixels based on distance from road. This process implies that managers have a specified minimum and maximum distance from road centerlines within which sampling can be conducted. ### roads #read roads shapefile roads &lt;- st_read(system.file(&quot;extdata/roads&quot;,&quot;roads.shp&quot;, package = &quot;RMFinventory&quot;)) ## Reading layer `roads&#39; from data source `G:\\Programs\\R-4.0.3\\library\\RMFinventory\\extdata\\roads\\roads.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 167 features and 36 fields ## geometry type: MULTILINESTRING ## dimension: XY ## bbox: xmin: 431100 ymin: 5337700 xmax: 438560 ymax: 5343240 ## projected CRS: UTM_Zone_17_Northern_Hemisphere roads &lt;- st_transform(roads, wall_crs@projargs) plot(roads[&quot;RDTYPE&quot;]) # Select only suitable road types # Highway (H), Municipal (M), Primary (P), Branch (B), Clay/mineral surface haul (C) and roads accessed when dry or frozen (d) , graveled (g), industrial grade road (i), highway or municipal road(r), yearly accessible (y) roads_subset &lt;- roads[roads$RDTYPE %in% c(&quot;H&quot;, &quot;M&quot;, &quot;P&quot;, &quot;B&quot;, &quot;C&quot;) &amp; roads$RDMOD %in% c(&quot;d&quot;, &quot;g&quot;, &quot;i&quot;, &quot;r&quot;, &quot;y&quot;), ] # Dissolve roads layer to work with only 1 feature roads_subset &lt;- st_union(roads_subset) plot(st_geometry(roads_subset)) #Create buffers of 30 m and 200 m roads_buffer_30m &lt;- st_buffer(roads_subset, dist = 30) roads_buffer_200m &lt;- st_buffer(roads_subset, dist = 200) #Take the symetrical difference between buffers to keep only roads &gt;- 30 m AND &lt;= 200 m roads_buffer &lt;- st_sym_difference(roads_buffer_200m, roads_buffer_30m) plot(st_geometry(roads_buffer)) #Transform to SpatialPolygonDataFrame object for compatibility with the raster package roads_buffer &lt;- sf::as_Spatial(roads_buffer) Road buffering outputs #Mask wall-to-wall layer wall_poly_roads &lt;- raster::mask(wall_poly, mask = roads_buffer) plot(wall_poly_roads$avg) ## Convert raster into data table for tabular representation of candidates rast_dt &lt;- as.data.table.raster(wall_poly_roads,xy=T) pre { max-height: 300px; overflow-y: auto; } pre[class] { max-height: 100px; } "],["princomp.html", "Principal Components Use PCA model to get PCA values of an existing set of plots Candidate cells Stratification Derive strata for existing plots Select new plots", " Principal Components The first method we outline is using principal components analysis. This vignette is replicated from the RMFInventory package by Martin Queinnec. The vignette using the following approach: Extract principal components from ALS metrics Extract PCA values for an existing plot network - in this case 5 plots Visualize PCAs and where existing plots fall Stratify PCAs using defined break points Extract strata for existing plot networks - determine which strata plots fall into Mask candidate cells using road buffer - improve efficiency of samples Use the sampleCells algorithm to representatively select sample cells within strata We are using the same raster data from the dataprep section. plot(wall_poly$avg) PCA &lt;- RStoolbox::rasterPCA(wall_poly, nComp = 2, spca = TRUE, maskCheck = FALSE) # nComp = 2, we return the two first principal components # spca = TRUE, since metrics have different ranges of values the fucntion will center and scale the metrics # maskCheck = FALSE, we don&#39;t check if some pixels have NA value in one or more layers. If not sure, set to TRUE # The output of rasterPCA is a list with an element model which contains the PCA model and an element map which contains the map of PCA values PCA_wall &lt;- PCA$map df_PCA_wall &lt;- as.data.frame(PCA_wall, na.rm = TRUE, xy = FALSE) pal &lt;- brewer.pal(n = 30, name = &quot;Spectral&quot;) plot(PCA_wall,col=pal) PCA_model &lt;- PCA$model We can check the proportion of variance contained in each principal component: summary(PCA_model) ## Importance of components: ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 ## Standard deviation 2.9333212 1.2809844 1.1476605 0.87050824 0.53135160 0.4227776 0.36171758 ## Proportion of Variance 0.6618749 0.1262247 0.1013173 0.05829112 0.02171804 0.0137493 0.01006459 ## Cumulative Proportion 0.6618749 0.7880996 0.8894169 0.94770797 0.96942602 0.9831753 0.99323990 ## Comp.8 Comp.9 Comp.10 Comp.11 Comp.12 Comp.13 ## Standard deviation 0.221817718 0.143761431 0.0973665642 0.0722469728 0.0513443058 2.597584e-02 ## Proportion of Variance 0.003784854 0.001589796 0.0007292498 0.0004015096 0.0002027875 5.190342e-05 ## Cumulative Proportion 0.997024754 0.998614550 0.9993437994 0.9997453091 0.9999480966 1.000000e+00 Use PCA model to get PCA values of an existing set of plots A network of 182 plots was already established in RMF. The objective of the structural guided sampling was to check if the existing plot network was covering the entire range of structural variability and if not, selecting new plots in underrepresented forest types. We load the existing set of plots. All the ALS metrics that were calculated to make the PCA model have also been calculated at the plot-level. Note: Make sure that the name of metrics used to get the PCA model correspond to the name of the plot-level metrics We can use the predict function to get PCA values of the existing set of plots and off the candidate cells # Plot-level plots_metrics &lt;- st_read(system.file(&quot;extdata&quot;, &quot;plots_metrics.shp&quot;, package = &quot;RMFinventory&quot;)) ## Reading layer `plots_metrics&#39; from data source `G:\\Programs\\R-4.0.3\\library\\RMFinventory\\extdata\\plots_metrics.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 5 features and 14 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: 432109.8 ymin: 5337921 xmax: 437372.2 ymax: 5342820 ## projected CRS: UTM_Zone_17_Northern_Hemisphere plots_metrics_coords &lt;- st_coordinates(plots_metrics) plots_metrics_df &lt;- plots_metrics %&gt;% st_drop_geometry() plots_PCA &lt;- as.data.frame(predict(PCA_model, plots_metrics_df))[,c(1,2)] colnames(plots_PCA) &lt;- c(&quot;PC1&quot;, &quot;PC2&quot;) Candidate cells We have produced the principal components for the wall-to-wall layers, but we also know that we want to limit plot selection based on road access. To do so we can use the PCA model from the wall-to-wall layer (as seen below), or we can simply use the the road buffered candidate ALS metrics as a mask. PCA_candidates &lt;- raster::predict(object = wall_poly_roads, model = PCA_model,index = c(1,2)) names(PCA_candidates) &lt;- c(&quot;PC1&quot;, &quot;PC2&quot;) df_PCA_candidates &lt;- as.data.frame(PCA_candidates, na.rm = TRUE, xy = FALSE) Once PCA values of all forested cells, candidate cells and existing plots have been calculated, it is useful to plot them to visualize their distribution. ## Function to calculate point density in scatter plot get_density &lt;- function(x, y, ...) { dens &lt;- MASS::kde2d(x, y, ...) ix &lt;- findInterval(x, dens$x) iy &lt;- findInterval(y, dens$y) ii &lt;- cbind(ix, iy) return(dens$z[ii]) } #Calculate point density for scatter plot visualization df_PCA_candidates &lt;- df_PCA_candidates %&gt;% mutate(dens = get_density(PC1, PC2, n = 300)) # Get convex hulls hulls_wall_idx &lt;- chull(df_PCA_wall$PC1, df_PCA_wall$PC2) hulls_wall &lt;- dplyr::slice(df_PCA_wall[,c(&quot;PC1&quot;,&quot;PC2&quot;)],hulls_wall_idx) hulls_cand_idx &lt;- chull(df_PCA_candidates$PC1, df_PCA_candidates$PC2) hulls_cand &lt;- dplyr::slice(df_PCA_candidates[,c(&quot;PC1&quot;,&quot;PC2&quot;)],hulls_cand_idx) ggplot(mapping = aes(x = PC1, y = PC2)) + geom_polygon(data = hulls_cand, colour = &quot;orange&quot;, fill = NA) + geom_point(data = df_PCA_candidates, aes(color = dens)) + scale_color_viridis_c() + geom_point(data = plots_PCA, color = &quot;red&quot;) + theme_bw() + coord_equal() + theme(panel.grid = element_blank(), legend.position = &quot;right&quot;) #Function to calculate point density in scatterplot get_density &lt;- function(x, y, ...) { dens &lt;- MASS::kde2d(x, y, ...) ix &lt;- findInterval(x, dens$x) iy &lt;- findInterval(y, dens$y) ii &lt;- cbind(ix, iy) return(dens$z[ii]) } Stratification Once PCA values have been calculated we can stratify the PC1 / PC2 feature space. The approach chosen was to stratify using equal intervals. From the previous plot of PCAs, we decided to stratify the feature space into 7 equal interval on the PC1 axis and 5 intervals on the PC2 axis - this makes a 7 x 5 grid - 35 unique strata. A challenge with this method as we will see is that many of these strata will be empty. To create the stratification we use the getPCAstrata function: # Determine the number of breaks for the first and second features (PC1 and PC2) strata &lt;- RMFinventory::getPCAstrata(PCA_layer = PCA_candidates, nbreaks = c(8, 6), #Since we want a 7 x 5 grid we need 8 x 6 breaks summary = TRUE) #getPCAstrata returns a list of three objects #raster strata_candidates &lt;- strata$strata_layer #breakpoints breaks &lt;- strata$breaks breaks ## $PC1 ## [1] -6.32285334 -4.24769842 -2.17254349 -0.09738857 1.97776635 4.05292128 6.12807620 8.20323113 ## ## $PC2 ## [1] -4.2217834 -1.9097285 0.4023263 2.7143812 5.0264361 7.3384910 #strata matrix - determines strata values strata$matrix ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] 11 21 31 41 51 61 71 ## [2,] 12 22 32 42 52 62 72 ## [3,] 13 23 33 43 53 63 73 ## [4,] 14 24 34 44 54 64 74 ## [5,] 15 25 35 45 55 65 75 We see the matrix has 35 unique strata values numbered by the PC1 and PC2 breaks. Some of these strata have no candidate cells meaning that, in reality, we have fewer than 35 strata. To resolve this issue we can remove all strata with 0 candidate cells and rename strata for better visual interpretation. plot_candidates &lt;- strata_candidates strata.df &lt;- strata$summary #this shows we have 27 unique strata out of the original 35. g &lt;- strata.df %&gt;% dplyr::filter(count &gt; 0) %&gt;% mutate(strata1 = seq(1,length(strata),1)) df.cand &lt;- data.frame(plot_candidates@data@values) names(df.cand) &lt;- c(&quot;strata&quot;) plot_candidates@data@values &lt;- g$strata1[match(df.cand$strata,g$strata)] pal &lt;- brewer.pal(n=27,name=&quot;Spectral&quot;) plot(plot_candidates,col = pal) Derive strata for existing plots We can use the breaks returned by the getPCAstrata function to get the strata of the existing set of plots: strata &lt;- getPCAstrata(PCA_layer = plots_PCA, breaks = breaks, # From the precedent call to getPCAstrata summary = TRUE) strata_plots &lt;- strata$strata_layer strata_plots ## [1] 42 12 54 52 54 ## 35 Levels: 11 &lt; 12 &lt; 13 &lt; 14 &lt; 15 &lt; 21 &lt; 22 &lt; 23 &lt; 24 &lt; 25 &lt; 31 &lt; 32 &lt; 33 &lt; 34 &lt; 35 &lt; 41 &lt; ... &lt; 75 Make a new plots with break lines between strata ggplot(mapping = aes(x = PC1, y = PC2)) + geom_polygon(data = hulls_cand, colour = &quot;orange&quot;, fill = NA) + geom_point(data = df_PCA_candidates, aes(color = dens)) + scale_color_viridis_c() + geom_point(data = plots_PCA, color = &quot;red&quot;) + geom_vline(xintercept = breaks$PC1, linetype = &quot;dashed&quot;) + geom_hline(yintercept = breaks$PC2, linetype = &quot;dashed&quot;) + theme_bw() + coord_equal() + theme(panel.grid = element_blank(), legend.position = &quot;right&quot;) Select new plots The function sampleCells that performs the sampling requires the number of cells to sample for each strata. This can be determined based on the previous feature space plots, total number of plots than can be selected, number of existing plots, how many of them should be re-measured etc. The following figure illustrates the sampling process: Figure 1: Sampling process diagram toSample &lt;- read.csv(system.file(&quot;extdata&quot;,&quot;cells_to_sample.csv&quot;, package = &quot;RMFinventory&quot;), stringsAsFactors = F) toSample ## strata toSample ## 1 13 1 ## 2 14 2 ## 3 22 1 ## 4 23 3 ## 5 24 3 ## 6 25 0 ## 7 31 1 ## 8 32 1 ## 9 33 5 ## 10 34 3 ## 11 35 0 ## 12 41 1 ## 13 42 2 ## 14 43 5 ## 15 44 3 ## 16 45 1 ## 17 51 1 ## 18 52 2 ## 19 53 4 ## 20 54 5 ## 21 55 2 ## 22 61 0 ## 23 62 1 ## 24 63 2 ## 25 64 2 ## 26 65 2 ## 27 72 0 ## 28 73 1 ## 29 74 1 ## 30 75 0 df.cand &lt;- data.frame(strata_candidates@data@values) names(df.cand) &lt;- c(&quot;strata&quot;) ns &lt;- 100 #total number of desired samples toSample &lt;- df.cand %&gt;% na.omit() %&gt;% group_by(strata) %&gt;% summarize(n= n()) %&gt;% mutate(freq = n / sum(n), samps = as.integer(freq*ns)) %&gt;% mutate(toSample = ifelse(samps == 0,1,samps)) %&gt;% dplyr::select(strata,toSample) %&gt;% as.data.frame() existing_plots &lt;- data.frame(plotID = plots_metrics$plotID, x = plots_metrics_coords[,1], y = plots_metrics_coords[,2], strata = strata_plots) existing_plots ## plotID x y strata ## 1 1 432109.8 5340150 42 ## 2 2 434310.2 5342820 12 ## 3 3 435506.5 5340228 54 ## 4 4 435478.0 5337921 52 ## 5 5 437372.2 5342478 54 new_plots &lt;- RMFinventory ::sampleCells(strata_layer = strata_candidates, matrix_strata = strata$matrix, existing_sample = existing_plots, # You can provide a set of existing plots or output of previous call to sampleCells and these cells won&#39;t be sampled again. toSample = toSample, mindist = 150, message = T) # There might be a warning like this: #no non-missing arguments to min; returning Infno non-missing arguments to max; returning -Inf # If think safe to ignore but need to look more into that new_plots ## x y strata cluster type plotID ## 2 434310.2 5342820 12 &lt;NA&gt; Existing 2 ## 1 435450.0 5342810 12 cluster New NA ## 11 434810.0 5341950 12 cluster New NA ## 12 437790.0 5342930 12 cluster New NA ## 13 437550.0 5343090 12 cluster New NA ## 14 433950.0 5341370 12 cluster New NA ## 15 438150.0 5337770 12 cluster New NA ## 16 434230.0 5342650 12 cluster New NA ## 17 437590.0 5339270 12 cluster New NA ## 18 437790.0 5338990 12 cluster New NA ## 19 435430.0 5342590 12 cluster New NA ## 110 436090.0 5342630 12 cluster New NA ## 111 437810.0 5342710 12 cluster New NA ## 112 435390.0 5342350 12 cluster New NA ## 113 436070.0 5342470 12 cluster New NA ## 114 438430.0 5337790 12 cluster New NA ## 115 437550.0 5338990 12 cluster New NA ## 116 438470.0 5339370 12 cluster New NA ## 117 433470.0 5341150 13 cluster extended New NA ## 118 433150.0 5343050 22 cluster New NA ## 21 437910.0 5339590 22 cluster New NA ## 3 438110.0 5339730 22 cluster New NA ## 4 438190.0 5339550 22 cluster New NA ## 5 435810.0 5342350 22 cluster New NA ## 6 438170.0 5340970 22 cluster New NA ## 7 435890.0 5339310 22 cluster New NA ## 119 433350.0 5341430 23 cluster New NA ## 22 434530.0 5339410 23 cluster New NA ## 31 435370.0 5338630 23 cluster New NA ## 41 435730.0 5338690 23 cluster New NA ## 51 433230.0 5341670 23 cluster New NA ## 61 438470.0 5339010 23 cluster New NA ## 71 438270.0 5338490 23 cluster extended New NA ## 120 435370.0 5340270 24 isolated New NA ## 121 435210.0 5337950 31 cluster extended New NA ## 122 437950.0 5341210 32 cluster New NA ## 23 432070.0 5342810 32 cluster New NA ## 32 436450.0 5339890 32 cluster New NA ## 42 437870.0 5339190 32 cluster New NA ## 52 438090.0 5340950 32 cluster New NA ## 62 435190.0 5337870 32 cluster New NA ## 72 435950.0 5342730 32 cluster New NA ## 8 434730.0 5342650 32 cluster New NA ## 9 431990.0 5342650 32 cluster New NA ## 10 432670.0 5342290 32 cluster New NA ## 123 435870.0 5342930 33 cluster New NA ## 24 434430.0 5338230 33 cluster extended New NA ## 33 433390.0 5342910 33 cluster extended New NA ## 43 435570.0 5343170 33 cluster extended New NA ## 53 437870.0 5341810 33 cluster extended New NA ## 124 434990.0 5338970 34 cluster extended New NA ## 125 432750.0 5339570 41 cluster extended New NA ## 126 432109.8 5340150 42 &lt;NA&gt; Existing 1 ## 1110 435290.0 5337850 42 cluster New NA ## 127 435470.0 5338050 42 cluster New NA ## 131 437610.0 5342330 42 cluster New NA ## 141 438330.0 5337850 42 cluster New NA ## 151 432230.0 5341430 42 cluster New NA ## 161 433250.0 5342650 42 cluster New NA ## 171 432990.0 5342910 42 cluster New NA ## 181 433450.0 5342030 42 cluster New NA ## 191 431890.0 5342530 42 cluster New NA ## 1101 433630.0 5342230 42 cluster New NA ## 1111 432990.0 5342090 42 cluster New NA ## 1121 432970.0 5342390 42 cluster New NA ## 1131 432550.0 5342090 42 cluster New NA ## 1141 437070.0 5341470 42 cluster New NA ## 128 435350.0 5338230 43 cluster extended New NA ## 25 438510.0 5338130 43 cluster extended New NA ## 34 431970.0 5337910 43 cluster extended New NA ## 44 437330.0 5341950 43 cluster extended New NA ## 54 434750.0 5343190 43 cluster extended New NA ## 129 437270.0 5342210 44 cluster extended New NA ## 130 435370.0 5340290 45 isolated New NA ## 132 434130.0 5341550 51 cluster New NA ## 26 432330.0 5339330 51 cluster New NA ## 35 438270.0 5338850 51 cluster New NA ## 45 435478.0 5337921 52 &lt;NA&gt; Existing 4 ## 133 436470.0 5341310 52 cluster New NA ## 1112 431790.0 5342750 52 cluster New NA ## 1210 432650.0 5340810 52 cluster New NA ## 134 434250.0 5337870 52 cluster New NA ## 142 436350.0 5340430 52 cluster New NA ## 152 434090.0 5337910 52 cluster New NA ## 162 436610.0 5341430 52 cluster New NA ## 172 436410.0 5341130 52 cluster New NA ## 182 432830.0 5340310 52 cluster New NA ## 192 431790.0 5343130 52 cluster New NA ## 1102 438250.0 5340210 52 cluster New NA ## 1113 436190.0 5339910 52 cluster New NA ## 1122 436550.0 5341730 52 cluster New NA ## 135 431570.0 5342730 53 cluster New NA ## 27 431870.0 5342910 53 cluster New NA ## 36 432190.0 5343210 53 cluster New NA ## 46 431330.0 5342750 53 cluster New NA ## 55 432770.0 5339150 53 cluster extended New NA ## 37 435506.5 5340228 54 &lt;NA&gt; Existing 3 ## 56 437372.2 5342478 54 &lt;NA&gt; Existing 5 ## 136 437250.0 5342230 54 cluster extended New NA ## 137 435430.0 5340250 55 cluster extended New NA ## 138 437330.0 5342690 61 isolated New NA ## 139 431710.0 5338410 62 cluster extended New NA ## 140 431950.0 5343090 63 cluster extended New NA ## 143 435490.0 5340270 64 cluster extended New NA ## 144 436590.0 5342970 65 cluster extended New NA ## 145 432950.0 5339690 73 cluster extended New NA ## 146 438450.0 5338670 74 cluster extended New NA ## 147 438470.0 5338690 75 cluster extended New NA #Assign plotID to sampled plots new_plots &lt;- new_plots %&gt;% mutate(plotID = ifelse(type == &quot;New&quot;, paste0(&quot;S_&quot;, seq_len(n())), plotID)) #Convert to sf object new_plots_sf &lt;- st_as_sf(new_plots, coords = c(&quot;x&quot;, &quot;y&quot;)) plot(strata_candidates) plot(st_geometry(new_plots_sf), add = T) #Get metrics and PCA of new plots new_plots_metrics &lt;- raster::extract(wall_poly, new_plots_sf) #could also get from point cloud directly new_plots_PCA &lt;- as.data.frame(predict(PCA_model, new_plots_metrics))[,c(1,2)] colnames(new_plots_PCA) &lt;- c(&quot;PC1&quot;, &quot;PC2&quot;) ggplot(mapping = aes(x = PC1, y = PC2)) + geom_polygon(data = hulls_cand, colour = &quot;orange&quot;, fill = NA) + geom_point(data = df_PCA_candidates, aes(color = dens)) + scale_color_viridis_c() + geom_point(data = new_plots_PCA, color = &quot;red&quot;) + geom_vline(xintercept = breaks$PC1, linetype = &quot;dashed&quot;) + geom_hline(yintercept = breaks$PC2, linetype = &quot;dashed&quot;) + theme_bw() + coord_equal() + theme(panel.grid = element_blank(), legend.position = &quot;right&quot;) pre { max-height: 300px; overflow-y: auto; } pre[class] { max-height: 100px; } "],["strat.html", "Other stratification methods K-means How many clusters? Optimum stratum boundaries Sampling within groups/strata Simple Random Sampling Using 2 variables to stratify Non-stratified sampling methods", " Other stratification methods The second method we outline is using k means stratification for consequent stratified sampling. K-means Here is a standard way to apply unsupervised K-means and allocate each ALS cell to a cluster. ### K MEANS ### set.seed(123) # Extract values for Kmeans v &lt;- rast_dt_wall ## determine index of each cell so to map values correctly without NA&#39;s idx &lt;- 1:ncell(wall_poly) idx &lt;- idx[-unique(which(is.na(v), arr.ind=TRUE)[,1])] ############################ #### K MEANS CLUSTERING #### ############################ #### DETERMINE OPTIMAL NUMBER OF CLUSTERS????? #Apply K means - center and scale values and remove NA&#39;s E &lt;- kmeans(na.omit(scale(v,center = T,scale=T)), 4) clusters &lt;- E$cluster # create empty raster from original candidate dimentions r.empty &lt;- wall_poly[[1]] r.empty[] &lt;- NA # add cluster values to corresponding index and re-name r.empty[idx] &lt;- clusters names(r.empty) &lt;- c(&quot;clusters&quot;) #combine clusters with original structural metrics wall_poly[[14]] &lt;- r.empty pal &lt;- brewer.pal(n = 4, name = &quot;Spectral&quot;) plot(wall_poly$clusters,col=pal) How many clusters? This is a loaded questions. How many clusters to choose can depends on many reasons, and there are many ways to determine how many to use. A challenge with the K-means method is being objective about the number of strata to use and where to partition the data. I present a number of examples below. (Papa et al. 2020) used k-means clustering and outlined that they used the Elbow method to determine the number of clusters. #Elbow Method for finding the optimal number of clusters set.seed(123) # data &lt;- as.data.frame(na.omit(scale(v))) # Elbow method fviz_nbclust(as.data.frame(na.omit(scale(v))), kmeans, method = &quot;wss&quot;) + geom_vline(xintercept = 4, linetype = 2) + # add line for better visualisation labs(subtitle = &quot;Elbow method&quot;) # add subtitle Based on the figure above we see that we can choose 4 clusters based on the elbow method. There are many methods that can be used. We see quickly that its not entirely cut and dry how to choose cluster numbers, but we have methods to decide that. This is likely something that the user will need to give input on. The NbClust() function from the (Charrad et al. 2015) package is an additional method, though it takes a long time to run (especially on large datasets). This function: provides 30 indices for determining the number of clusters and proposes to user the best clustering scheme from the different results obtained by varying all combinations of number of clusters, distance measures, and clustering methods. nb &lt;- NbClust(data, diss=NULL, distance = &quot;euclidean&quot;, min.nc=2, max.nc=12, method = &quot;kmeans&quot;, index = &quot;all&quot;, alphaBeale = 0.1) Optimum stratum boundaries A method to address this is using optimal break methods like that of strata.data() from the (Reddy and Khan 2019) package. This method takes the population of candidate cells v and a fixed sample size 100 to compute the optimum stratum boundaries (OSB) for a given number of strata. Along with OSB it also provides the optimum sample sizes within strata directly from the data. In this example we specify that we would like data to be split into 4 strata and we iterate splits on the avg cov and p99 variables. Some studies like (Maltamo et al. 2011) and (Hawbaker et al. 2009) used multiple metrics to split on, while others like used only 1. #prepare data o_val &lt;- v %&gt;% na.omit() %&gt;% as.data.frame() ########################################## ####### OPTIMAL CLUSTERING BREAKS ######## ########################################## ##### Sampling within clusters OSB &lt;- list() for(i in c(1,2,10)){ #1 - average height #2 - covariance #10 - p99 height res &lt;- strata.data(o_val[,i], h = 4, n=100) k &lt;- names(o_val[1]) breaks &lt;- data.frame(res$OSB) colnames(breaks)=k OSB[[i]] &lt;- breaks } breaks &lt;- cbind(as.data.frame(OSB[[1]]),as.data.frame(OSB[[2]]),as.data.frame(OSB[[10]])) names(breaks) &lt;- c(&quot;avg&quot;,&quot;cov&quot;,&quot;p99&quot;) saveRDS(breaks,&quot;dat/breaks.RDS&quot;) ggplot(o_val,aes(avg))+ geom_histogram()+ geom_vline(xintercept = breaks$avg, linetype = &quot;dashed&quot;) Sampling within groups/strata Once OSB are defined we can associate the groups with the data themselves and perform statistical tests to determine wherther groups are significantly difference from one another. (Papa et al. 2020) used ANOVA and Tukey post-hoc tests. First, we can change the resulting group labels from cut() to more understandable characters. #define groups within splitting variable split_osb &lt;- c(-Inf,breaks$avg[1:3],Inf) grps &lt;- cut(o_val$avg,split_osb) o_val$groups &lt;- as.factor(grps) #convert groups to A | B | C | D o_val &lt;- o_val %&gt;% mutate(groups = case_when(groups == &quot;(-Inf,5.1]&quot;~&quot;A&quot;, groups == &quot;(5.1,9.33]&quot;~&quot;B&quot;, groups == &quot;(9.33,14.4]&quot;~&quot;C&quot;, groups == &quot;(14.4, Inf]&quot;~&quot;D&quot;)) # Statistical testing summary(mod1 &lt;- aov(avg~groups,data=o_val)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## groups 3 1290629 430210 266539 &lt;2e-16 *** ## Residuals 87230 140794 2 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 o_val %&gt;% ggplot(aes(groups,avg,fill=groups)) + stat_eye() Once these strata are established we can begin to test a variety of sampling mechanisms. Simple Random Sampling ## Without road masking ns &lt;- 200 #total number of desired samples samples &lt;- o_val %&gt;% group_by(groups) %&gt;% summarize(n= n()) %&gt;% mutate(freq = n / sum(n), plot_tot = as.integer(freq*ns)) df_sample &lt;- o_val %&gt;% group_by(groups) %&gt;% arrange(groups) %&gt;% nest() %&gt;% ungroup() %&gt;% mutate(n = samples$plot_tot) %&gt;% mutate(samp = map2(data, n, sample_n)) %&gt;% dplyr::select(-data) %&gt;% unnest(samp) # sample summary statistics df_sample %&gt;% group_by(groups) %&gt;% dplyr::select(avg,groups) %&gt;% summarise( n = n(), mean = mean(avg), sd = sd(avg) ) ## # A tibble: 4 x 4 ## groups n mean sd ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 41 3.34 1.00 ## 2 B 60 7.20 1.12 ## 3 C 79 11.7 1.39 ## 4 D 18 15.9 0.883 #population summary statistics o_val %&gt;% group_by(groups) %&gt;% dplyr::select(avg,groups) %&gt;% summarise( n = n(), mean = mean(avg), sd = sd(avg) ) ## # A tibble: 4 x 4 ## groups n mean sd ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 17933 3.30 1.05 ## 2 B 26395 7.36 1.23 ## 3 C 34626 11.8 1.41 ## 4 D 8280 15.7 1.23 coords &lt;- st_as_sf(df_sample, coords = c(&quot;x&quot;,&quot;y&quot;)) plot(wall_poly$avg) plot(st_geometry(coords),add=T) ggplot(mapping = aes(x=avg,y=std))+ geom_point(data = o_val, aes(color = groups))+ geom_point(data = df_sample, pch=21, fill = &quot;red&quot;,colour=&quot;black&quot;)+ theme_bw() + ylim(0,15) + coord_equal() + theme(panel.grid = element_blank(), legend.position = &quot;right&quot;) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## groups 3 532594 177531 117528 &lt;2e-16 *** ## Residuals 29898 45162 2 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ### SRS Road Masked {-} ns &lt;- 200 #total number of desired samples samples &lt;- o_val %&gt;% group_by(groups) %&gt;% summarize(n= n()) %&gt;% mutate(freq = n / sum(n), plot_tot = as.integer(freq*ns)) df_sample &lt;- o_val %&gt;% group_by(groups) %&gt;% arrange(groups) %&gt;% nest() %&gt;% ungroup() %&gt;% mutate(n = samples$plot_tot) %&gt;% mutate(samp = map2(data, n, sample_n)) %&gt;% dplyr::select(-data) %&gt;% unnest(samp) # sample summary statistics df_sample %&gt;% group_by(groups) %&gt;% dplyr::select(avg,groups) %&gt;% summarise( n = n(), mean = mean(avg), sd = sd(avg) ) ## # A tibble: 4 x 4 ## groups n mean sd ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 61 2.88 0.875 ## 2 B 50 6.96 1.20 ## 3 C 69 11.8 1.44 ## 4 D 18 15.6 0.961 #population summary statistics o_val %&gt;% group_by(groups) %&gt;% dplyr::select(avg,groups) %&gt;% summarise( n = n(), mean = mean(avg), sd = sd(avg) ) ## # A tibble: 4 x 4 ## groups n mean sd ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 9126 3.15 0.992 ## 2 B 7590 7.29 1.23 ## 3 C 10404 11.9 1.41 ## 4 D 2782 15.7 1.21 Visualizing the samples coords &lt;- st_as_sf(df_sample, coords = c(&quot;x&quot;,&quot;y&quot;)) plot(wall_poly_roads$avg) plot(st_geometry(coords),add=T) ggplot(mapping = aes(x=avg,y=std))+ geom_point(data = o_val, aes(color = groups))+ geom_point(data = df_sample, pch=21, fill = &quot;red&quot;,colour=&quot;black&quot;)+ theme_bw() + ylim(0,15) + coord_equal() + theme(panel.grid = element_blank(), legend.position = &quot;right&quot;) Using 2 variables to stratify ## Without road masking # Extract values for Kmeans v &lt;- rast_dt_wall o_val &lt;- v %&gt;% na.omit() %&gt;% as.data.frame() #define groups within splitting variable split_osb &lt;- c(-Inf,breaks$avg[1:3],Inf) grps &lt;- cut(o_val$avg,split_osb) o_val$groups &lt;- as.character(grps) #convert groups to A | B | C | D o_val &lt;- o_val %&gt;% mutate(groups = case_when(groups == &quot;(-Inf,5.1]&quot;~1, groups == &quot;(5.1,9.33]&quot;~2, groups == &quot;(9.33,14.4]&quot;~3, groups == &quot;(14.4, Inf]&quot;~4)) o_val &lt;- o_val %&gt;% group_by(groups) %&gt;% mutate(groups2 = ntile(std,3)) %&gt;% unite(&quot;groups_m&quot;,groups,groups2) %&gt;% mutate(id = group_indices(., groups_m)) ns &lt;- 200 #total number of desired samples samples &lt;- o_val %&gt;% group_by(groups_m) %&gt;% summarize(n= n()) %&gt;% mutate(freq = n / sum(n),plot_tot = as.integer(freq*ns)) samp &lt;- o_val %&gt;% group_by(groups_m) %&gt;% arrange(groups_m) %&gt;% nest() %&gt;% ungroup() %&gt;% mutate(n = samples$plot_tot) %&gt;% mutate(samp = map2(data, n, sample_n)) %&gt;% dplyr::select(-data) %&gt;% unnest(samp) coords &lt;- st_as_sf(samp, coords = c(&quot;x&quot;,&quot;y&quot;)) plot(wall_poly$avg) plot(st_geometry(coords),add=T) p &lt;- ggplot(mapping = aes(x=avg,y=std))+ geom_point(data = o_val, aes(color = groups_m))+ geom_point(data = samp, pch=21, fill = &quot;red&quot;,colour=&quot;black&quot;)+ theme_bw() + coord_equal() + theme(panel.grid = element_blank(), legend.position = &quot;right&quot;) coordsgrps &lt;- o_val %&gt;% group_by(groups_m) %&gt;% arrange(groups_m) %&gt;% nest() %&gt;% ungroup() p + geom_rect(data=coordsgrps$data[[1]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[2]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[3]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[4]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[5]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[6]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[7]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[8]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[9]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[10]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[11]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[12]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA) 2 Variable Road Masked ## With road masking # Extract values for Kmeans v &lt;- rast_dt o_val &lt;- v %&gt;% na.omit() %&gt;% as.data.frame() #define groups within splitting variable split_osb &lt;- c(-Inf,breaks$avg[1:3],Inf) grps &lt;- cut(o_val$avg,split_osb) o_val$groups &lt;- as.character(grps) #convert groups to A | B | C | D o_val &lt;- o_val %&gt;% mutate(groups = case_when(groups == &quot;(-Inf,5.1]&quot;~1, groups == &quot;(5.1,9.33]&quot;~2, groups == &quot;(9.33,14.4]&quot;~3, groups == &quot;(14.4, Inf]&quot;~4)) o_val &lt;- o_val %&gt;% group_by(groups) %&gt;% mutate(groups2 = ntile(std,3)) %&gt;% unite(&quot;groups_m&quot;,groups,groups2) %&gt;% mutate(id = group_indices(., groups_m)) ns &lt;- 200 #total number of desired samples samples &lt;- o_val %&gt;% group_by(groups_m) %&gt;% summarize(n= n()) %&gt;% mutate(freq = n / sum(n),plot_tot = as.integer(freq*ns)) samp &lt;- o_val %&gt;% group_by(groups_m) %&gt;% arrange(groups_m) %&gt;% nest() %&gt;% ungroup() %&gt;% mutate(n = samples$plot_tot) %&gt;% mutate(samp = map2(data, n, sample_n)) %&gt;% dplyr::select(-data) %&gt;% unnest(samp) coords &lt;- st_as_sf(samp, coords = c(&quot;x&quot;,&quot;y&quot;)) plot(wall_poly_roads$avg) plot(st_geometry(coords),add=T) p &lt;- ggplot(mapping = aes(x=avg,y=std))+ geom_point(data = o_val, aes(color = groups_m))+ geom_point(data = samp, pch=21, fill = &quot;red&quot;,colour=&quot;black&quot;)+ theme_bw() + coord_equal() + theme(panel.grid = element_blank(), legend.position = &quot;right&quot;) coordsgrps &lt;- o_val %&gt;% group_by(groups_m) %&gt;% arrange(groups_m) %&gt;% nest() %&gt;% ungroup() p + geom_rect(data=coordsgrps$data[[1]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[2]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[3]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[4]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[5]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[6]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[7]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[8]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[9]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[10]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[11]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA)+ geom_rect(data=coordsgrps$data[[12]],aes(xmin=min(avg),xmax=max(avg),ymin=min(std),ymax=max(std)),colour=&quot;black&quot;,fill=NA) # Convert dataframe back to raster to show classes spg &lt;- o_val coordinates(spg) &lt;- ~ x + y # coerce to SpatialPixelsDataFrame gridded(spg) &lt;- TRUE # coerce to raster rasterDF &lt;- raster(spg, &quot;id&quot;) #plot pal &lt;- brewer.pal(n = 12, name = &quot;Spectral&quot;) ## Warning in brewer.pal(n = 12, name = &quot;Spectral&quot;): n too large, allowed maximum for palette Spectral is 11 ## Returning the palette you asked for with that many colors plot(rasterDF,col=pal) plot(st_geometry(coords),add=T) Non-stratified sampling methods Some sampling methods such as the BalancedSampling approaches and Latin Hypercube Sampling do not require prior stratification. In order to sample these we present these methods on only candidate samples within the road buffer subsets. #prepare data set.seed(123) # Extract values for Kmeans v &lt;- rast_dt o_val &lt;- v %&gt;% na.omit() %&gt;% as.data.frame() Balanced Sampling (Grafström and Ringvall 2013) described the (Grafström and Lisic 2019) package, which implements a number of sampling methods that balance samples spatially and within auxilary variable space. The spatial balance is important given that we library(BalancedSampling) # Example 1 set.seed(123); N = nrow(o_val); # population size n = 200; # sample size p = rep(n/N,N);# inclusion probabilities X = as.matrix(o_val) # matrix of auxiliary variables s1 = lpm1(p,X); # select sample plot(X[,3],X[,5]); # plot population points(X[s1,3],X[s1,5], pch=19,col=&quot;red&quot;); # plot sample plot(X[,1],X[,2]); # plot population points(X[s1,1],X[s1,2], pch=19,col=&quot;red&quot;); # plot sample s2 = lpm2(p,X); # select sample plot(X[,3],X[,5]); # plot population points(X[s2,3],X[s2,5], pch=19,col=&quot;red&quot;); # plot sample plot(X[,1],X[,2]); # plot population points(X[s2,1],X[s2,2], pch=19,col=&quot;red&quot;); # plot sample s3 = lcube(p,X,cbind(p)); plot(X[,1],X[,2]); # plot population points(X[s3,1],X[s3,2], pch=19,col=&quot;red&quot;); # plot sample plot(X[,3],X[,5]); # plot population points(X[s3,3],X[s3,5], pch=19,col=&quot;red&quot;); # plot sample sampls1 &lt;- as.data.frame(X[s1,]) %&gt;% mutate(alg = &quot;lmp1&quot;) sampls2 &lt;- as.data.frame(X[s2,]) %&gt;% mutate(alg = &quot;lmp2&quot;) sampls3 &lt;- as.data.frame(X[s3,]) %&gt;% mutate(alg = &quot;lcube&quot;) samps &lt;- rbind(sampls1,sampls2,sampls3) samps %&gt;% group_by(alg) %&gt;% dplyr::select(avg) %&gt;% summarise( n = n(), mean = mean(avg), sd = sd(avg) ) ## # A tibble: 3 x 4 ## alg n mean sd ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 lcube 200 8.26 4.32 ## 2 lmp1 200 8.26 4.51 ## 3 lmp2 200 8.22 4.50 o_val %&gt;% dplyr::select(avg) %&gt;% summarise( n = n(), mean = mean(avg), sd = sd(avg) ) ## n mean sd ## 1 29902 8.396322 4.395713 Latin Hypercube Sampling There are a number of different approaches to take with Latin Hypercube Sampling. The following code was derived from (Malone, Minansy, and Brungard 2019), who present a method to boostrap sample number for hypercube sampling, followed by a few testing options to determine optimal sample numbers. This code was provided by the authors at https://bitbucket.org/brendo1001/clhc_sampling/downloads/ which i have manipulated to work with our dataset. #################################################################### # Data analysis for the population data #Principal components of the population (the is for tests 1 and 2) pca1 = prcomp(df[,3:6],scale=TRUE, center=TRUE) scores_pca1 = as.data.frame(pca1$x) screeplot(pca1) ## plot the variances explained by each component biplot(pca1) summary(pca1) # retreive the loadings pca1.load&lt;- matrix(NA,ncol=4,nrow=4 ) for (i in 1:4){ pca1.load[i,]&lt;- as.matrix(t(pca1$rotation[i,]))} #Quantiles of the population (this is for test 3) # Number of bins nb&lt;- 25 #quantile matrix (of the covariate data) q.mat&lt;- matrix(NA, nrow=(nb+1), ncol= 4) j=1 for (i in 3:ncol(df)){ #note the index start here #get a quantile matrix together of the covariates ran1&lt;- max(df[,i]) - min(df[,i]) step1&lt;- ran1/nb q.mat[,j]&lt;- seq(min(df[,i]), to = max(df[,i]), by =step1) j&lt;- j+1} q.mat #covariate data hypercube (this is for test 4) ## This takes a while to do so only do it once if you can cov.mat&lt;- matrix(1, nrow=nb, ncol=4) for (i in 1:nrow(df)){ # the number of pixels cntj&lt;- 1 for (j in 3:ncol(df)){ #for each column dd&lt;- df[i,j] for (k in 1:nb){ #for each quantile kl&lt;- q.mat[k, cntj] ku&lt;- q.mat[k+1, cntj] if (dd &gt;= kl &amp; dd &lt;= ku){cov.mat[k, cntj]&lt;- cov.mat[k, cntj] + 1} } cntj&lt;- cntj+1 } } cov.mat #################################################################### roads &lt;- st_read(system.file(&quot;extdata/roads&quot;,&quot;roads.shp&quot;, package = &quot;RMFinventory&quot;)) roads &lt;- st_transform(roads, wall_crs@projargs) ####################################################################### #How many samples do we need? #beginning of algorithm #initial settings cseq&lt;- seq(10,500,10) # cLHC sample size its&lt;-10 # number internal iterations with each sample size number mat.seq&lt;- matrix(NA,ncol=8,nrow=length(cseq)) #empty matix for outputs for (w in 1:length(cseq)){ # for every sample number configuration.... s.size=cseq[w] # sample size mat.f&lt;- matrix(NA,ncol=8,nrow=its ) # placement for iteration outputs #internal loop for (j in 1:its){ #Note that this takes quite a while to run to completion repeat{ ss &lt;- clhs(df[,3:6], size = s.size, progress = T, iter = 100) # Do a conditioned latin hypercube sample s.df&lt;- df[ss,] if (sum(duplicated(s.df) | duplicated(s.df[nrow(s.df):1, ])[nrow(s.df):1]) &lt; 2) {break}} #principal component of sample pca.s = prcomp(s.df[,3:6],scale=TRUE, center=TRUE) scores_pca1 = as.data.frame(pca.s$x) # plot the first 2 principal components and convex hull rand.tr&lt;-tri.mesh(scores_pca1[,1],scores_pca1[,2]) rand.ch&lt;-convex.hull(rand.tr, plot.it=F) #convex hull pr_poly = cbind(x=c(rand.ch$x),y=c(rand.ch$y)) # save the convext hull vertices #plot(scores_pca1[,1], scores_pca1[,2], xlab=&quot;PCA 1&quot;, ylab=&quot;PCA 2&quot;, xlim=c(min(scores_pca1[,1:2]), max(scores_pca1[,1:2])),ylim=c(min(scores_pca1[,1:2]), max(scores_pca1[,1:2]))) #lines(c(rand.ch$x,rand.ch$x[1]), c(rand.ch$y,rand.ch$y[1]),col=&quot;red&quot;,lwd=1) # draw the convex hull(domain of prediction) #### First Test: #points in polygons routine # PCA prjection PCA_projection&lt;- predict(pca.s, df[,3:6]) # Project population onto sample PC newScores = cbind(x=PCA_projection[,1],y=PCA_projection[,2]) # PC scores of projected population #plot the polygon and all points to be checked #plot(newScores,xlab=&quot;PCA 1&quot;, ylab=&quot;PCA 2&quot;, xlim=c(min(newScores[,1:2]), max(newScores[,1:2])),ylim=c(min(newScores[,1:2]), max(newScores[,1:2])),col=&#39;black&#39;, main=&#39;convex hull of ss&#39;) # polygon(pr_poly,col=&#39;#99999990&#39;) #create check which points fall within the polygon specMatch = pip2d(pr_poly,newScores) specMatch = specMatch &gt; 0 mat.f[j,7]&lt;- sum(specMatch)/length(specMatch)*100 # propertion of new spectra the fall within the convex hull # points(specMatch[which(specMatch==0),1:2],pch=&#39;X&#39;, col=&#39;red&#39;) ##END points in polygons## #### Second Test: #similarity of the 2 matrices (PCA Similarity Factor; Krzanowski (1979)) # retreive the loadings for the samples pca.s.load&lt;- matrix(NA,ncol=4,nrow=4 ) for (i in 1:4){ pca.s.load[i,]&lt;- as.matrix(t(pca.s$rotation[i,])) } # Perfrom the Krznowski 1979 calculation ps1&lt;- pca1.load[,1:2] ps2&lt;- pca.s.load[,1:2] ps1.t&lt;- t(ps1) #transpose ps2.t&lt;- t(ps2) #transpose S&lt;- ps1.t %*% ps2 %*% ps2.t %*% ps1 mat.f[j,1]&lt;-sum(diag(S))/2 ## Third Test: #comparison of quantiles df.q2.1&lt;-quantile(s.df[,3], probs = seq(0, 1, 0.25),names = F, type = 7) df.q1.1&lt;-quantile(df[,3], probs = seq(0, 1, 0.25),names = F, type = 7) mat.f[j,2]&lt;-sqrt((df.q1.1[1]-df.q2.1[1])^2 + (df.q1.1[2]-df.q2.1[2])^2 + (df.q1.1[3]-df.q2.1[3])^2 + (df.q1.1[4]-df.q2.1[4])^2 ) df.q2.2&lt;-quantile(s.df[,4], probs = seq(0, 1, 0.25),names = F, type = 7) df.q1.2&lt;-quantile(df[,4], probs = seq(0, 1, 0.25),names = F, type = 7) mat.f[j,3]&lt;-sqrt((df.q1.2[1]-df.q2.2[1])^2 + (df.q1.2[2]-df.q2.2[2])^2 + (df.q1.2[3]-df.q2.2[3])^2 + (df.q1.2[4]-df.q2.2[4])^2 ) df.q2.3&lt;-quantile(s.df[,5], probs = seq(0, 1, 0.25),names = F, type = 7) df.q1.3&lt;-quantile(df[,5], probs = seq(0, 1, 0.25),names = F, type = 7) mat.f[j,4]&lt;-sqrt((df.q1.3[1]-df.q2.3[1])^2 + (df.q1.3[2]-df.q2.3[2])^2 + (df.q1.3[3]-df.q2.3[3])^2 + (df.q1.3[4]-df.q2.3[4])^2 ) df.q2.4&lt;-quantile(s.df[,6], probs = seq(0, 1, 0.25),names = F, type = 7) df.q1.4&lt;-quantile(df[,6], probs = seq(0, 1, 0.25),names = F, type = 7) mat.f[j,5]&lt;-sqrt((df.q1.4[1]-df.q2.4[1])^2 + (df.q1.4[2]-df.q2.4[2])^2 + (df.q1.4[3]-df.q2.4[3])^2 + (df.q1.4[4]-df.q2.4[4])^2 ) mat.f[j,6]&lt;-mean(mat.f[j,2:5]) # take the mean distance ## Fourth test: Kullback-Leibler (KL) divergence ####Compare whole study area covariate space with the selected sample #sample data hypercube (essentially the same script as for the grid data but just doing it on the sample data) h.mat&lt;- matrix(1, nrow=nb, ncol=4) for (ii in 1:nrow(s.df)){ # the number of observations cntj&lt;- 1 for (jj in 3:ncol(s.df)){ #for each column dd&lt;- s.df[ii,jj] for (kk in 1:nb){ #for each quantile kl&lt;- q.mat[kk, cntj] ku&lt;- q.mat[kk+1, cntj] if (dd &gt;= kl &amp; dd &lt;= ku){h.mat[kk, cntj]&lt;- h.mat[kk, cntj] + 1} } cntj&lt;- cntj+1}} #h.mat #Kullback-Leibler (KL) divergence klo.1&lt;- KL.empirical(c(cov.mat[,1]), c(h.mat[,1])) #1 klo.2&lt;- KL.empirical(c(cov.mat[,2]), c(h.mat[,2])) #2 klo.3&lt;- KL.empirical(c(cov.mat[,3]), c(h.mat[,3])) #3 klo.4&lt;- KL.empirical(c(cov.mat[,4]), c(h.mat[,4])) #4 klo&lt;- mean(c(klo.1, klo.2,klo.3,klo.4)) mat.f[j,8]&lt;- klo # value of 0 means no divergence } #arrange outputs mat.seq[w,1]&lt;-mean(mat.f[,6]) mat.seq[w,2]&lt;-sd(mat.f[,6]) mat.seq[w,3]&lt;-min(mat.f[,1]) mat.seq[w,4]&lt;-max(mat.f[,1]) mat.seq[w,5]&lt;-mean(mat.f[,7]) mat.seq[w,6]&lt;-sd(mat.f[,7]) mat.seq[w,7]&lt;-mean(mat.f[,8]) mat.seq[w,8]&lt;-sd(mat.f[,8]) } ## END of LOOP dat.seq&lt;- as.data.frame(cbind(cseq,mat.seq)) names(dat.seq)&lt;- c(&quot;samp_nos&quot;, &quot;mean_dist&quot;,&quot;sd_dist&quot;, &quot;min_S&quot;, &quot;max_S&quot;, &quot;mean_PIP&quot;,&quot;sd_PIP&quot;, &quot;mean_KL&quot;,&quot;sd_KL&quot;) ########################################################## After the loop has finished we are able to plot the ouputs of the bootstrapping to outline how the sample size influenced the deviation of population and sample statistics. ####################################################### #plot some outputs plot(cseq,mat.seq[,1], xlab=&quot;number of samples&quot;, ylab= &quot;similarity between covariates (entire field) with covariates (sample)&quot;,main=&quot;Population and sample similarity&quot;) plot(cseq,mat.seq[,2],xlab=&quot;number of samples&quot;, ylab= &quot;standard deviation similarity between covariates (entire field) with covariates (sample)&quot;,main=&quot;Population and sample similarity (sd)&quot;) plot(cseq,mat.seq[,3]) plot(cseq,mat.seq[,4]) plot(cseq,mat.seq[,5],xlab=&quot;number of samples&quot;, ylab= &quot;percentage of total covariate variance of population account for in sample&quot;,main=&quot;Population and sample similarity&quot;) plot(cseq,mat.seq[,6],xlab=&quot;number of samples&quot;, ylab= &quot;standard deviation of percentage of total covariate variance of population account for in sample&quot;,main=&quot;Population and sample similarity&quot;) plot(cseq,mat.seq[,7],xlab=&quot;number of samples&quot;, ylab= &quot;KL divergence&quot;) plot(cseq,mat.seq[,8],xlab=&quot;number of samples&quot;, ylab= &quot;standard deviation of percentage of total covariate variance of population account for in sample&quot;,main=&quot;Population and sample similarity&quot;) ########################################################## ########################################################## # make an exponetial decay function (of the KL divergence) x&lt;- dat.seq$samp_nos y = 1- (dat.seq$mean_PIP-min(dat.seq$mean_PIP))/(max(dat.seq$mean_PIP)-min(dat.seq$mean_PIP)) #PIP #Parametise Exponential decay function plot(x, y, xlab=&quot;sample number&quot;, ylab= &quot;1 - PC similarity&quot;) # Initial plot of the data start &lt;- list() # Initialize an empty list for the starting values #fit 1 manipulate( { plot(x, y) k &lt;- kk; b0 &lt;- b00; b1 &lt;- b10 curve(k*exp(-b1*x) + b0, add=TRUE) start &lt;&lt;- list(k=k, b0=b0, b1=b1) }, kk=slider(0, 5, step = 0.01, initial = 2), b10=slider(0, 1, step = 0.000001, initial = 0.01), b00=slider(0,1 , step=0.000001,initial= 0.01)) fit1 &lt;- nls(y ~ k*exp(-b1*x) + b0, start = start) summary(fit1) lines(x, fitted(fit1), col=&quot;red&quot;) After determining the disparity between samples and population we can optimize the number of samples we use based on the cumulative frequency distribution. We determined that the optimum number of samples was 119. #Apply fit xx&lt;- seq(1, 500,1) lines(xx, predict(fit1,list(x=xx))) jj&lt;- predict(fit1,list(x=xx)) normalized = 1- (jj-min(jj))/(max(jj)-min(jj)) x&lt;- xx y&lt;- normalized plot(x, y, xlab=&quot;sample number&quot;, ylab= &quot;normalised PIP&quot;, type=&quot;l&quot;, lwd=2) # Initial plot of the data x1&lt;- c(-1, 500) y1&lt;- c(0.95, 0.95) lines(x1,y1, lwd=2, col=&quot;red&quot;) num &lt;- data.frame(xx,normalized) %&gt;% filter(abs(normalized - 0.95) == min(abs(normalized - 0.95))) %&gt;% dplyr::select(xx) %&gt;% pull() paste0(&quot;your optimum sample size is: &quot;,num) x2&lt;- c(num, num); y2&lt;- c(0, 1) lines(x2,y2, lwd=2, col=&quot;red&quot;) ############################################################################# ##END set.seed(2021) df &lt;- rast_dt %&gt;% dplyr::select(x,y,avg,std,cov,p99) %&gt;% na.omit() %&gt;% as.data.frame() ss &lt;- clhs(df[,3:6], size = 143, progress = T, iter = 10) # Do a conditioned latin hypercube sample ## | | | 0% | |========== | 11% | |==================== | 22% | |============================== | 33% | |======================================== | 44% | |=================================================== | 56% | |============================================================= | 67% | |======================================================================= | 78% | |================================================================================= | 89% | |===========================================================================================| 100% s.df&lt;- df[ss,] coords &lt;- st_as_sf(s.df, coords = c(&quot;x&quot;,&quot;y&quot;)) plot(wall_poly_roads$avg) plot(st_geometry(coords),add=T) This may look well and good.. But its important to look at WHERE the samples are being located to see if the algorithm was effective. See the image below showing that that we likely need to introduce a spatial aspect to the sampling. Not really sure how to do this just yet pre { max-height: 300px; overflow-y: auto; } pre[class] { max-height: 100px; } References "],["disc.html", "Discussion points Methods Need to integrate already established samples Algorithm creation Agenda for future", " Discussion points Methods Do we think that all the methods presented are realistic and valuable. Some methods have more wiggle room for sample selection, while others can be more rigid due to their implementation within previous functions. Need to discuss the methods and determine which should be included/explored in our final product. Need to integrate already established samples Incorporation of already acquired sample plots needs to be incorporated into the structurally guided sampling protocol - if available. There are a number of potential avenues to do this including: Assess number of acquired samples within strata and incorporate values into needed sample sizes - i.e like in RMFinventory Evaluate whether strata are over/under sampled using available algorithms - Quantiles, spatial distribution (thresholds?) etc. The following is derived from (Malone, Minansy, and Brungard 2019), who outline the use of the hypercube evaluation of legacy sample (HELS) algorithm original presented in (Carré, McBratney, and Minasny 2007)) for determining where additional samples are required in the presence of already available samples. Essentially the goal of adapted HELS is to identify the environmental coverage of existing sample sites, and simultaneously identifying where there are gaps in the environmental data that are not covered by these same existing sample sites. Once this is achieved, new sample sites are added - where the user selects how many sites they can afford to get - and these are allocated preferentially to areas where existing sample coverage is most sparse. The conditioned latin hypercube code provided does not introduce already acquired samples, though it does have the functionality to do so using the include paramter. This needs to be explored further. A good vignette describing this along side a comparison the HELS algorithm is presented here. Algorithm creation With a basis established in the RMFinventory package, I have begun scripting some of the methods described in this brief overview of what I hope the package will entail. With guidance from Martin Q I have a good understanding of what I need to do to start to script the different methods together. Agenda for future In terms of future steps I think it wise that we discuss the following: Stratification approaches - do these suffice - do we know of more / improved methods? Sampling procedures - do these suffice - do we know of more / improved methods? Scripting approach - I am not a software engineer nor a statistician Alot of this scripting is new to me so any advice that resources/collaboration would be highly valued. Datasets to use - Which datasets should we use to test the approaches - prefferable having alread established sampling protocols Publishing - What testing / comparisons do we want to do to show the efficacy of this project. This could also lead well into #3. References "],["references.html", "References", " References "]]

[["index.html", "SGS Overview", " SGS Tristan Goodbody 2021-03-11 Overview This document is a brief introduction to the struturally guided sampling project. Herein i provide a: Literature review about SGS Overview of how data has been prepared Work done in the RMFinventory package using principle components Potential methods to apply using stratification and sampling methods Points to discuss for how the project should evolve "],["SGS.html", "Structurally Guided Sampling Review Stratification approaches", " Structurally Guided Sampling Review (McRoberts, Gobakken, and Næsset 2012) outline the essence of structurally guided sampling: Stratified estimation is a statistical technique that can be used to increase the precision of estimates without increasing sample sizes. The essence of stratified estimation is to aggregate observations of the response variable into groups or strata that are more homogeneous than the population as a whole. The population mean and its variance are estimated as weighted means ofwithin-strata means and variances where the weights are based on strata sizes. If the stratification is accomplished prior to sampling and the within-stratum variances are known or can be easily estimated, then greater precision may be achieved by selecting within-strata sampling intensities to be proportional to within-strata variances (Cochran, 1977). Following literature review I present a number of implementations that have been used to guide the stratification of ALS-derived structural variables for locating representative sample plots. Studies including (Leiterer et al. 2015) and (Kane et al. 2010) suggest that the information in the ALS data may be condensed to a few metrics. Many of the studies reviewed seem to adopt this by selecting a low number of metrics which they use to stratify the landscape and consequently sample within.(Fedrigo et al. 2018) further outlined: Lidar metrics may be useful for forest stand classification if they differentiate classes of interest (Wulder et al., 2008). The separation of the data into vertical layers allows metrics to be derived for specific forest strata, which may increase the discrimination of multi-strata stand types. The selection of breaks that define these strata can be based on field knowledge or identified statistically from lidar profiles (Lefsky et al., 1999; Zhang et al., 2011). An additional note of interest is that a great deal of research in the field of digital soil mapping can be applied to structurally guided sampling. Studies including (Carré, McBratney, and Minasny 2007), (Ma et al. 2020), and (Malone, Minansy, and Brungard 2019) were invlauable for understanding how to use auxillary variables to determine effective sampling protocols - especially in the context of Lahin Hypercube Sampling. Stratification approaches (Hawbaker et al. 2009) Used ALS mean and SD to stratify. First they separated forest types then 10 strata were derived suing mean height. Within each stratum, 3 additional sub-strata were created using SD. This produced a total of 30 stratifications where one coniferous forest and two deciduous forest locations randomly selected. They compared samples taken from strata with a random sample finding that: &gt;Prediction errors from models constructed using a random sample were up to 68% larger than those from the models built with a stratified sample. The stratified sample included a greater range of variability than the random sample. (Dalponte et al. 2011) They analyzed 2 aspects of ground reference data collection. The positioning error of the ground plots The optimal number of training plots. They had a large collection of samples &gt;700 and systematically analyzed how the reduction of sample numbers influenced accuracy of attribute estimates. They tested the effect of error in estimate related to distance the center point of the plot was moved. The decrease was at a minimum when the displacement was less than 5 m, while it was considerable over 10 m. They did random downsampling of the sample plot number and stratified downsampling of the sample plot number. They used the methods from (Hawbaker et al. 2009) and (Maltamo et al. 2011) for stratified downsampling. Comparing the results obtained with the proposed stratified sampling method and the other three methods considered (see Table 3), one can see that the proposed stratified sampling method was the one that provided the best results for both sets. The new protocol allowed us to obtain promising results for the considered dataset: using only 53 training plots, instead of 534 in the original dataset, we obtained the same results for the validation set. (Maltamo et al. 2011) Used divided P90 into 12 even parts. A near equal number of sample plots were systematically chosen within each of these 12 parts according to the range of VEG (ratio of vegetation points and ground points). To representatively sample the 12 parts and to cover the extremes, selection intensity was higher in the p90 range range with the lowest number of previously established plots. The use of ALS data as a priori information provided the most accurate results. They also provide the an important statement: One should keep in mind that in real world applications ,some given ALS variables will have to be used for selection of plots regardless of which and how many biophysical properties the inventory aims at estimating. Thus, some ALS variables will work better for certain biophysical properties than for others. An appropriate way of choosing suitable ALS variables could be to select them according to the relative importance of the biophysical properties to be estimated.for volume and number of stems. plots. REduced total sample size incrementally. (Junttila et al. 2013) Used a number of approaches including a space filling design, which attempts to sample uniformly across a feature space. The Maximin design attempts to cover a feature space as uniformly as possible given a fixed number of sample points. It maximizes the minimum distance between plots. The results indicate that choosing the plots in a way that ensures ample coverage of both spatial and feature space variability improves the performance of the corresponding models, and that adequate coverage of the variability in the feature space is the most important condition that should be met by the set of plots collected. (Valbuena et al. 2013) Used the Cover metric to privde a idea of the density of each forest area. The justified that cover influenced the relation between other metrics and the forest response and, therefore, it was included in all the models computed. (Grafström and Ringvall 2013) Presents the (Grafström and Lisic 2019) package. and does comparisons between Simple random sampling (SRS) and: Local pivotal method with x and y coordinates (LPM-xy) to make sure the sample is well spread geographically. The Euclidean distance is used without standardization because the spatial coordinates are on the same scale. Local pivotal method (LPM) XY and four selected ALS variables as auxiliary variables. The samples are well spread geographically and well spread in the four ALS variables. The distance function (eq. (3)) is used. Cube method (CM) The samples are balanced on the four ALS variables. The x and y coordinates are not used for this method, because it does not make sense to balance on geographical coordinates. Local cube method (LCM) The samples are balanced on the four ALS variables and the x and y coordinates are used to make sure the sample is well spread geographically. The Euclidean distance is used in the geographical space. They found that spreading the sample well in geographical space is important. Spreading the sample well in all auxiliary variables (LPM) and using the HT estimator was in all cases a more efficient strategy than SRS followed by a calibration estimator. Spreading the sample well is more efficient than balancing (or SRS followed by calibration) if the relationship between the target and the auxiliary variables is nonlinear, which is why it performs better (see Tables 2 and 3). The final order of the designs combined with the calibration estimator was (from best to worst) LCM, LPM, LPM-xy, CM, and SRS. We get the best results when using the information both in the design and the estimator. Hence, we can conclude that using auxiliary information in the design is very important. \" All three designs, LCM, LPM, and CM, had very good design effects. The reduction of the variance of the HT estimator was for the main target variable volume up to 75% compared with SRS. (Grafström, Saarela, and Ene 2014) Continuation of previous study highlighting value of sampling methods. A possible drawback of the LPM compared with more simple methods such as systematic random sampling is that sample se- lection is computationally intensive for large populations. This is a problem because the population size (number of pixels) tends to be rather large in forest applications with auxiliary data from, e.g., airborne LiDAR, making it practically impossible to apply the LPM as it is. To deal with this problem, we introduce a new rapid suboptimal implementation ofthe LPM that can be applied to large populations. With the LPM, there is no stratification or other difficult design choices; we only need to define distance. Modelling is not really necessary, which means no complicated formulas. Yet this is a very competitive strategy in many cases. It has the advantage of being intuitive and simple, which should not be underrated. It also allows us to keep the traditional design-based approach, while still taking advantage of powerful auxiliary information. With the LPM, there is no stratification or other difficult design choices; we only need to define distance. Modelling is not really necessary, which means no complicated formulas. Yet this is a very competitive strategy in many cases. It has the advantage of being intuitive and simple, which should not be underrated. It also allows us to keep the traditional design-based approach, while still taking advantage of powerful auxiliary information. (Niemi and Vauhkonen 2016) Used ALS textural CHM metrics (HIST,PATCH,GLCM,LBP) for pre-stratification of the inventory area. They tested the strength of the relationship between metrics and central attributes of forest growing stock (Vol,BA,Diam). It was found that the dispersion of the clusters derived in an unsupervised mode could be used as an indicator for prioritizing the plots to be measured as the sample to form the reference data for the wall-to-wall models. (Valbuena et al. 2017) Used a rule based method to stratify forest areas using Lmoments including Lcv and Lskew. (McRoberts, Chen, and Walters 2017) Modeling inventory attributes with ALS metrics. BA was considered as an integrator of all the response variables and was used as the basis for constructing strata using a lonear model. Predictions of BA were then scaled by the largest BA prediction,multiplied by 100, then assigned 1 of 100 potential strata. Strata were then aggregated these into 4,6,8 strata with approximately equal proportion of study area. Stratifications that are most effective with respect to minimizing variances are based on variables that are closely related to the response variable or variables of interest. When multiple response variables are to be estimated simultaneously, the same stratification must be used for all response variables to ensure compatibility. Results showed they were able to: &gt;Reduce sampling intensity by up to 35% without losing accuracy in the estimators. Relative to the simple random sampling estimators that used no stratification, use of four strata decreased SEs by 29.0%38.1%, depending on the response variable; relative to four strata, use of six strata reduced SEs by 2.3%7.3%; and relative to six strata, use of eight strata reduced SEs by no more than 4% and for two response variables actually increased SEs. From an operational perspective, the results of the study suggest that multivariate, airborne laser scanning-assisted inventories could be fairly easily implemented, subject to availability of the airborne laser scanning data. (Malone, Minansy, and Brungard 2019) They provide an indepth description of how to implement and improve the outputs of Latin Hypercube sampling, providing a coded example of how to incorporate already acquired samples accross the landscape. They specify a method to boostrap sample size followed by the application of multiple methods to determine the optimal sample number to use based on auxillary data. (Papa et al. 2020) Compared simple random sampling to stratified sampling within strata groups. Used hierarchical cluster analysis for formation of strata. Clusters were used for sub-sampling. The number of clusters was established by the Elbow method. Clusters were validated with an ANOVA post-hoc Tukey test. Varying size of metric cells (1, 0.5, 0.25 ha) were tested. The stratified sampling was simulated by using weighting distributed by strata, so that each segment was the same sample size, whereas no weighting was used for the simulation in the case of the simple random sampling. For each level of sampling size (number of plots in the inventory), 1000 iterations were performed, and their mean and standard deviation were summarized in order to produce a graph of relative uncertainty (standard error/global mean) as a function of sample intensity (proportion of total area sampled, in %). (Ma et al. 2020) They compare latin hypercube sampling and feature space coverage sampling for predicting soil classes. They detail specific functions to use for latin hypercube sampling and feature space coverage sampling. Sampling pools of varying sizes are compared to simple random sampling in a number of modeling approaches. Final results indicated that: 1)In both study areas the median overall accuracy with FSCS washigher than those with CLHS and SRS over all sample sizes and crossall three prediction methods. The median overall accuracy with CLHS was only marginally larger than with SRS. 2)There was a significant negative correlation betweenMSSSDandoverall accuracy, whereas no such correlation was found betweenO1 + O3and overall accuracy.The coefficient of variation in overall accuracy among samples se-lected using FSCS was smaller than these using CLHS and SRS at thesame sample sizes. 3)With CLHS the variation in overall accuracy among samples waslarge, so that there is a serious risk that a particular sample mightlead to a low overall accuracy. 4)FSCS-RF is the most accurate combination of sampling and predic-tion for both study areas Results indicated that feature space coverage sampling could be a viable method for locating samples. References "],["dataprep.html", "Data Preparation", " Data Preparation To show the sampling methods in action I use the example data from the Romeo mallete forest in Ontario. These data are included in the (???) package. These data were prepared suing the following method: ## Reading layer `inventory_polygons&#39; from data source `G:\\Programs\\R-4.0.3\\library\\RMFinventory\\extdata\\inventory_polygons\\inventory_polygons.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 632 features and 111 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 431100 ymin: 5337700 xmax: 438560 ymax: 5343240 ## projected CRS: UTM_Zone_17_Northern_Hemisphere ## Reading layer `roads&#39; from data source `G:\\Programs\\R-4.0.3\\library\\RMFinventory\\extdata\\roads\\roads.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 167 features and 36 fields ## geometry type: MULTILINESTRING ## dimension: XY ## bbox: xmin: 431100 ymin: 5337700 xmax: 438560 ymax: 5343240 ## projected CRS: UTM_Zone_17_Northern_Hemisphere These are the final candidate samples from which we will perform our sampling. "],["princomp.html", "Principle Components Use PCA model to get PCA values of an existing set of plots Stratification Select new plots", " Principle Components The first method we outline is using principle components analysis and sample selection from the (it 2021) package. This vignette is mostly replicated from the one within that package by Martin Queinnec. We can check the proportion of variance contained in each principal component: ## Importance of components: ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 ## Standard deviation 2.983183 1.2064618 1.13300103 0.85484659 0.51901897 ## Proportion of Variance 0.684568 0.1119654 0.09874549 0.05621252 0.02072159 ## Cumulative Proportion 0.684568 0.7965334 0.89527885 0.95149136 0.97221296 ## Comp.6 Comp.7 Comp.8 Comp.9 ## Standard deviation 0.41358876 0.325794942 0.21689690 0.140456117 ## Proportion of Variance 0.01315813 0.008164796 0.00361879 0.001517532 ## Cumulative Proportion 0.98537108 0.993535880 0.99715467 0.998672202 ## Comp.10 Comp.11 Comp.12 Comp.13 ## Standard deviation 0.0969461455 0.0732846305 0.0427731550 2.574170e-02 ## Proportion of Variance 0.0007229658 0.0004131259 0.0001407341 5.097195e-05 ## Cumulative Proportion 0.9993951681 0.9998082940 0.9999490281 1.000000e+00 Use PCA model to get PCA values of an existing set of plots A network of 182 plots was already established in RMF. The objective of the structural guided sampling was to check if the existing plot network was covering the entire range of structural variability and if not, selecting new plots in underrepresented forest types. We load the existing set of plots. All the ALS metrics that were calculated to make the PCA model have also been calculated at the plot-level. Note: Make sure that the name of metrics used to get the PCA model correspond to the name of the plot-level metrics We can use the predict function to get PCA values of the existing set of plots and off the candidate cells ## Reading layer `plots_metrics&#39; from data source `G:\\Programs\\R-4.0.3\\library\\RMFinventory\\extdata\\plots_metrics.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 5 features and 14 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: 432109.8 ymin: 5337921 xmax: 437372.2 ymax: 5342820 ## projected CRS: UTM_Zone_17_Northern_Hemisphere Once PCA values of all forested cells, candidate cells and existing plots have been calculated, it is useful to plot them to visualize their distribution. Stratification Once PCA values have been calculated we can stratify the PC1 / PC2 feature space. The approach chosen for the RMF was to stratify using equal intervals. From the previous plot, we can decide to stratify the feature space into 7 equal interval on the PC1 axis and 5 intervals on the PC2 axis (7 x 5 grid). We can use the function getPCAstrata to automatically create the stratification: ## class : RasterLayer ## dimensions : 277, 373, 103321 (nrow, ncol, ncell) ## resolution : 20, 20 (x, y) ## extent : 431100, 438560, 5337700, 5343240 (xmin, xmax, ymin, ymax) ## crs : +proj=utm +zone=17 +ellps=GRS80 +units=m +no_defs ## source : memory ## names : strata ## values : 12, 75 (min, max) ## $PC1 ## [1] -5.3840455 -3.4519431 -1.5198408 0.4122616 2.3443639 4.2764663 6.2085686 ## [8] 8.1406710 ## ## $PC2 ## [1] -4.1297172 -1.8145574 0.5006024 2.8157622 5.1309220 7.4460818 ## # A tibble: 35 x 3 ## strata count frac ## &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 11 0 0 ## 2 12 5467 0.183 ## 3 13 130 0.00435 ## 4 14 0 0 ## 5 15 0 0 ## 6 21 0 0 ## 7 22 3183 0.106 ## 8 23 1619 0.0541 ## 9 24 0 0 ## 10 25 0 0 ## # ... with 25 more rows ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] 11 21 31 41 51 61 71 ## [2,] 12 22 32 42 52 62 72 ## [3,] 13 23 33 43 53 63 73 ## [4,] 14 24 34 44 54 64 74 ## [5,] 15 25 35 45 55 65 75 We can use the breaks returned by the getPCAstrata function to get the strata of the existing set of plots: ## [1] 42 12 54 52 54 ## 35 Levels: 11 &lt; 12 &lt; 13 &lt; 14 &lt; 15 &lt; 21 &lt; 22 &lt; 23 &lt; 24 &lt; 25 &lt; 31 &lt; 32 &lt; ... &lt; 75 ## # A tibble: 35 x 3 ## strata count frac ## &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 11 0 0 ## 2 12 1 0.2 ## 3 13 0 0 ## 4 14 0 0 ## 5 15 0 0 ## 6 21 0 0 ## 7 22 0 0 ## 8 23 0 0 ## 9 24 0 0 ## 10 25 0 0 ## # ... with 25 more rows Make a new plots with break lines between strata Select new plots The function sampleCells that performs the sampling requires the number of cells to sample for each strata. This can be determined based on the previous feature space plots, total number of plots than can be selected, number of existing plots, how many of them should be re-measured etc. The following figure illustrates the sampling process: Figure 1: Sampling process diagram ## strata toSample ## 1 13 1 ## 2 14 2 ## 3 22 1 ## 4 23 3 ## 5 24 3 ## 6 25 0 ## 7 31 1 ## 8 32 1 ## 9 33 5 ## 10 34 3 ## 11 35 0 ## 12 41 1 ## 13 42 2 ## 14 43 5 ## 15 44 3 ## 16 45 1 ## 17 51 1 ## 18 52 2 ## 19 53 4 ## 20 54 5 ## 21 55 2 ## 22 61 0 ## 23 62 1 ## 24 63 2 ## 25 64 2 ## 26 65 2 ## 27 72 0 ## 28 73 1 ## 29 74 1 ## 30 75 0 ## plotID x y strata ## 1 1 432109.8 5340150 42 ## 2 2 434310.2 5342820 12 ## 3 3 435506.5 5340228 54 ## 4 4 435478.0 5337921 52 ## 5 5 437372.2 5342478 54 ## x y strata cluster type plotID ## 1 438450.0 5339210 13 cluster extended New NA ## 2 438230.0 5339710 22 cluster New NA ## 3 438490.0 5339010 23 cluster New NA ## 4 435350.0 5338630 23 cluster New NA ## 5 434530.0 5339410 23 cluster New NA ## 6 437990.0 5341250 32 cluster New NA ## 7 434970.0 5339230 33 cluster extended New NA ## 8 435430.0 5340070 33 cluster extended New NA ## 9 438030.0 5338830 33 cluster extended New NA ## 10 433410.0 5342950 33 cluster extended New NA ## 11 432570.0 5339090 33 cluster extended New NA ## 12 434390.0 5339190 34 isolated New NA ## 13 435110.0 5339610 34 isolated New NA ## 14 437490.0 5342910 34 isolated New NA ## 15 433410.0 5342550 41 cluster extended New NA ## 16 432109.8 5340150 42 &lt;NA&gt; Existing 1 ## 111 436490.0 5341690 42 cluster New NA ## 121 433450.0 5342030 42 cluster New NA ## 17 438250.0 5341310 43 cluster extended New NA ## 21 437250.0 5342150 43 cluster extended New NA ## 31 431230.0 5342710 43 cluster extended New NA ## 41 438370.0 5341070 43 cluster extended New NA ## 51 438290.0 5337930 43 cluster extended New NA ## 18 435030.0 5338950 44 isolated New NA ## 22 437410.0 5342590 44 isolated New NA ## 32 434390.0 5339510 44 isolated New NA ## 19 432510.0 5338850 51 cluster New NA ## 42 435478.0 5337921 52 &lt;NA&gt; Existing 4 ## 110 436230.0 5341430 52 cluster New NA ## 112 432850.0 5340310 52 cluster New NA ## 113 431550.0 5342730 53 cluster New NA ## 23 432190.0 5343210 53 cluster New NA ## 33 433070.0 5339410 53 cluster extended New NA ## 43 434330.0 5342950 53 cluster extended New NA ## 34 435506.5 5340228 54 &lt;NA&gt; Existing 3 ## 52 437372.2 5342478 54 &lt;NA&gt; Existing 5 ## 114 432990.0 5339110 54 cluster extended New NA ## 115 437190.0 5342170 54 cluster extended New NA ## 122 437290.0 5342350 54 cluster extended New NA ## 131 438390.0 5340810 54 cluster extended New NA ## 141 432830.0 5338950 54 cluster extended New NA ## 116 437290.0 5342430 55 cluster extended New NA ## 24 435470.0 5340290 55 cluster extended New NA ## 117 438330.0 5341010 62 cluster extended New NA ## 118 431890.0 5338510 63 cluster extended New NA ## 25 432730.0 5340070 63 cluster extended New NA ## 119 432990.0 5339590 64 cluster extended New NA ## 26 438150.0 5341910 64 cluster extended New NA ## 120 437310.0 5342410 65 cluster extended New NA ## 27 437950.0 5341590 65 cluster extended New NA ## 123 432710.0 5341030 73 cluster extended New NA ## 124 438450.0 5338710 74 cluster extended New NA References "],["strat.html", "Stratification methods K-means How many clusters?? Optimum stratum boundaries Simple Random Sampling Using 2 variables to stratify Balanced Sampling Latin Hypercube Sampling", " Stratification methods The second method we outline is using k means stratification for consequent stratified sampling. K-means Here is a standard way to apply unsupervised K-means and allocate each ALS cell to a cluster. How many clusters?? This is a loaded questions. How many clusters to choose can depends on many reasons, and there are many ways to determine how many to use. A challenge with the K-means method is being objective about the number of strata to use and where to partition the data. I present a number of examples below. (Papa et al. 2020) used k-means clustering and outlined that they used the Elbow method to determine the number of clusters. Based on the figure above we see that we can choose 4 clusters based on the elbow method. Other methods also exist including the Silhoutte method. Based on the figure above the Silhouette method suggests we use 2 clusters. We see quickly that its not entirely cut and dry how to choose cluster numbers, but we have methods to decide that. This is likely something that the user will need to give input on. The NbClust() function from the (Charrad et al. 2015) package is an additional method, though it takes a long time to run (especially on large datasets). This function provides 30 indices for determining the number of clusters and proposes to user the best clustering scheme from the different results obtained by varying all combinations of number of clusters, distance measures, and clustering methods. Optimum stratum boundaries A method to address this is using optimal break methods like that of strata.data() from the (Reddy and Khan 2019) package. This method takes the population of candidate cells v and a fixed sample size 100 to compute the optimum stratum boundaries (OSB) for a given number of strata. Along with OSB it also provides the optimum sample sizes within strata directly from the data. In this example we specify that we would like data to be split into 4 strata and we iterate splits on the avg cov and p99 variables. Some studies like (Maltamo et al. 2011) and (Hawbaker et al. 2009) used multiple metrics to split on, while others like used only 1. ## Sampling within groups/strata {-} Once OSB are defined we can associate the groups with the data themselves and perform statistical tests to determine wherther groups are significantly difference from one another. (Papa et al. 2020) used ANOVA and Tukey post-hoc tests. First, we can change the resulting group labels from cut() to more understandable characters. ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## groups 3 532594 177531 117528 &lt;2e-16 *** ## Residuals 29898 45162 2 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Once these strata are established we can begin to test a variety of sampling mechanisms. Simple Random Sampling ## # A tibble: 4 x 4 ## groups n mean sd ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 61 3.31 1.07 ## 2 B 50 7.42 1.12 ## 3 C 69 11.8 1.42 ## 4 D 18 15.5 0.925 ## # A tibble: 4 x 4 ## groups n mean sd ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 9126 3.15 0.992 ## 2 B 7590 7.29 1.23 ## 3 C 10404 11.9 1.41 ## 4 D 2782 15.7 1.21 Visualizing the samples Using 2 variables to stratify Balanced Sampling (Grafström and Ringvall 2013) described the (Grafström and Lisic 2019) package, which implements a number of sampling methods that balance samples spatially and within auxilary variable space. The spatial balance is important given that we ## # A tibble: 3 x 4 ## alg n mean sd ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 lcube 200 8.26 4.32 ## 2 lmp1 200 8.26 4.51 ## 3 lmp2 200 8.22 4.50 ## # A tibble: 1 x 3 ## n mean sd ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 29902 8.40 4.40 Latin Hypercube Sampling There are a number of different approaches to take with Latin Hypercube Sampling. The following code was derived from (Malone, Minansy, and Brungard 2019), who present a method to boostrap sample number for hypercube sampling, followed by a few testing options to determine optimal sample numbers. This code was provided by the authors at https://bitbucket.org/brendo1001/clhc_sampling/downloads/ which i have manipulated to work with our dataset. After the loop has finished we are able to plot the ouputs of the bootstrapping to outline how the sample size influenced the deviation of population and sample statistics. After determining the disparity between samples and population we can optimize the number of samples we use based on the cumulative frequency distribution. We determined that the optimum number of samples was 119. ## | | | 0% | |======== | 11% | |================ | 22% | |======================== | 33% | |================================ | 44% | |======================================= | 56% | |=============================================== | 67% | |======================================================= | 78% | |=============================================================== | 89% | |=======================================================================| 100% This may look well and good.. But its important to look at WHERE the samples are being located to see if the algorithm was effective. See the image below showing that that we likely need to introduce a spatial aspect to the sampling. Not really sure how to do this just yet References "],["disc.html", "Discussion points Methods Need to integrate already established samples Algorithm creation Agenda for future", " Discussion points Methods Do we think that all the methods presented are realistic and valuable. Some methods have more wiggle room for sample selection, while others can be more rigid due to their implementation within previous functions. Need to discuss the methods and determine which should be included/explored in our final product. Need to integrate already established samples Incorporation of already acquired sample plots needs to be incorporated into the structurally guided sampling protocol - if available. There are a number of potential avenues to do this including: Assess number of acquired samples within strata and incorporate values into needed sample sizes - i.e like in RMFinventory Evaluate whether strata are over/under sampled using available algorithms - Quantiles, spatial distribution (thresholds?) etc. The following is derived from (Malone, Minansy, and Brungard 2019), who outline the use of the hypercube evaluation of legacy sample (HELS) algorithm original presented in (Carré, McBratney, and Minasny 2007)) for determining where additional samples are required in the presence of already available samples. Essentially the goal of adapted HELS is to identify the environmental coverage of existing sample sites, and simultaneously identifying where there are gaps in the environmental data that are not covered by these same existing sample sites. Once this is achieved, new sample sites are added - where the user selects how many sites they can afford to get - and these are allocated preferentially to areas where existing sample coverage is most sparse. The conditioned latin hypercube code provided does not introduce already acquired samples, though it does have the functionality to do so using the include paramter. This needs to be explored further. A good vignette describing this along side a comparison the HELS algorithm is presented here. Algorithm creation With a basis established in the RMFinventory package, I have begun scripting some of the methods described in this brief overview of what I hope the package will entail. With guidance from Martin Q I have a good understanding of what I need to do to start to script the different methods together. Agenda for future In terms of future steps I think it wise that we discuss the following: Stratification approaches - do these suffice - do we know of more / improved methods? Sampling procedures - do these suffice - do we know of more / improved methods? Scripting approach - I am not a software engineer nor a statistician Alot of this scripting is new to me so any advice that resources/collaboration would be highly valued. Datasets to use - Which datasets should we use to test the approaches - prefferable having alread established sampling protocols Publishing - What testing / comparisons do we want to do to show the efficacy of this project. This could also lead well into #3. References "],["references.html", "References", " References "]]
